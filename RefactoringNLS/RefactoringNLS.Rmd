---
title: "Refactoring the `nls()` function in R"
author: 
   - John C. Nash \thanks{ retired professor, Telfer School of Management, University of Ottawa}
   - Arkajyoti Bhattacharjee \thanks{Department of Mathematics and Statistics, Indian Institute of Technology, Kanpur}
date: "2022-6-15"
output: 
    pdf_document:
        keep_tex: true
        toc: true
bibliography: ../BibSupport/ImproveNLS.bib
link-citations: yes
linkcolor: red
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## require(bookdown) # language engine to display text - does not seem necessary
```

<!-- ?? nlme::gnls() -- should we note it? -->

# Abstract

Based on work for a Google Summer of Code project "Improvements to `nls()`",
we consider the features and limitations of the R function `nls()`
in the context of improving and rationalizing R tools for nonlinear regression. 

Important considerations are the usability and  maintainability of
the code base that provides the functionality `nls()` claims to offer.
Our work suggests that the
existing code makes maintenance and improvement very difficult,
with legacy applications and examples blocking some important updates.
Discussion of these matters is relevant to improving R generally, as
well as its nonlinear estimation tools.

# The `nls()` function: strengths and shortcomings

`nls()` is the tool in base R, the primary software software distribution
from the Comprehensive R Archive Network (https://cran.r-project.org)
for estimating nonlinear statistical models. The function dates to the 1980s and the work
related to @bateswatts in S (see https://en.wikipedia.org/wiki/S_%28programming_language%29). 

The `nls()` function has a remarkable and comprehensive set of capabilities for estimating
nonlinear models that are expressed as **formulas**. In particular, we note that it

- handles formulas that include R functions, even ones which call calculations in
  other programming languages
- allows data to be weighted or subset
- can estimate bound constrained parameters
- provides a mechanism for handling partially linear models
- permits parameters to be indexed over a set of related data
- produces measures of variability (i.e., standard error estimates) for the estimated parameters
- has related profiling capabilities for exploring the likelihood surface as parameters are changed
- links to a number of pre-coded ("selfStart") models 

With such a range of features and long history, the code has become untidy 
and overly patched. It is, to
our mind, essentially unmaintainable. Moreover, its underlying methods can and should be improved, 
as we suggest below. 

## Feature: Convergence and termination tests (FIXED)

A previous issue with `nls()` that prevented it from providing parameter estimates for zero-residual
(i.e., perfect fit) data was corrected thanks to suggestions by one of us. 

In the manual page for `nls()` in R 4.0.0 there is the warning:

>  **Do not use `nls` on artificial "zero-residual" data.**

>  The `nls` function uses a relative-offset convergence criterion
  that compares the numerical imprecision at the current parameter
  estimates to the residual sum-of-squares.  This performs well on data of
  the form 
  
>  $$  y = f(x, \theta) + eps $$
  
>  (with `var(eps) > 0`).  It fails to indicate convergence on data of the form

>  $$  y = f(x, \theta)  $$ 
  
>  because the criterion amounts to comparing two components of the round-off 
  error.

>  If you wish to test `nls` on artificial data please add a noise component, 
  as shown in the example below.

This amounted to admitting R cannot solve well-posed problems unless data is polluted
with errors. 

This issue can be easily resolved. The "termination test" for the **program** rather than for 
"convergence" of the underlying **algorithm** is the Relative Offset Convergence Criterion 
(see @BatesWatts81). This projects the proposed step in the parameter vector on the gradient 
and estimates how much the sum of squares loss function should decrease. This estimate is divided 
by the current size of the loss function to avoid scale issues.
When we have "converged", the estimated
decrease is very small. However, with small residuals,
the sum of squares loss function is (almost) zero and we get the possibility of a 
zero-divide failure. 

Adding a small quantity to the loss function before dividing avoids trouble. 
In 2021, one of us (J. Nash) proposed that `nls.control()` have an additional parameter `scaleOffset` 
with a default value of zero. Setting it to a small number -- 1.0 is 
a reasonable choice --
allows small-residual problems (i.e., near-exact fits) to be dealt with easily. We call this the
**safeguarded relative offset convergence criterion**. The default value gives the legacy behaviour.
We are pleased to note that this improvement is now in the R distributed code as of version 4.1.0.

<!-- ### Example of a small-residual problem -->

<!-- ``` -->
<!-- t <- -10:10 -->
<!-- y <- 100/(1 + .1 * exp(-0.51 * t)) -->
<!-- lform <- y ~ a/(1 + b * exp(-c * t)) -->
<!-- ldata <- data.frame(t = t, y = y) -->
<!-- plot(t, y) -->
<!-- lstartbad <- c(a = 1, b = 1, c = 1) -->
<!-- lstart2 <- c(a = 100, b = 10, c = 1) -->
<!-- nlsr::nlxb(lform, data = ldata, start = lstart2) -->
<!-- nls(lform, data = ldata, start = lstart2, trace = TRUE) -->
<!-- # Fix with scaleOffset -->
<!-- nls(lform, data = ldata, start = lstart2, trace = TRUE, control = list(scaleOffset = 1.0)) -->
<!-- sessionInfo() -->
<!-- ``` -->

<!-- Edited output of as small example follows: -->

<!-- ``` -->
<!-- > t <- -10:10 -->
<!-- > y <- 100/(1 + .1 * exp(-0.51 * t)) -->
<!-- > lform <- y ~ a/(1 + b * exp(-c * t)) -->
<!-- > ldata <- data.frame(t=t, y=y) -->
<!-- > plot(t,y) -->
<!-- > lstartbad <- c(a = 1, b = 1, c = 1) -->
<!-- > lstart2 <- c(a = 100, b = 10, c = 1) -->
<!-- > nlsr::nlxb(lform, data = ldata, start = lstart2) -->
<!-- nlsr object: x  -->
<!-- residual sumsquares =  1.007e-19  on  21 observations -->
<!--     after  13    Jacobian and  19 function evaluations -->
<!--   name            coeff          SE       tstat      pval      gradient    JSingval    -->
<!-- a                    100     2.679e-11  3.732e+12  1.863e-216  -6.425e-11       626.6   -->
<!-- b                    0.1      3.78e-13  2.646e+11  9.125e-196  -3.393e-08       112.3   -->
<!-- c                   0.51       6.9e-13  7.391e+11  8.494e-204   1.503e-08       2.791   -->
<!-- > nls(lform, data = ldata, start = lstart2, trace = TRUE) -->
<!-- 40346.    (1.08e+00): par = (100 10 1) -->
<!-- 11622.    (2.93e+00): par = (101.47 0.49449 0.71685) -->
<!-- 5638.0    (1.08e+01): par = (102.23 0.38062 0.52792) -->
<!-- 642.08    (1.04e+01): par = (102.16 0.22422 0.41935) -->
<!-- 97.712    (1.79e+01): par = (100.7 0.14774 0.45239) -->
<!-- 22.250    (1.78e+02): par = (99.803 0.093868 0.50492) -->
<!-- 0.025789  (1.33e+03): par = (100.01 0.10017 0.50916) -->
<!-- 6.0571e-08 (7.96e+05): par = (100 0.1 0.51) -->
<!-- 4.7017e-19 (1.86e+04): par = (100 0.1 0.51) -->
<!-- 1.2440e-27 (5.71e-01): par = (100 0.1 0.51) -->
<!-- 1.2440e-27 (5.71e-01): par = (100 0.1 0.51) -->
<!-- ... (approx 40 lines omitted) -->
<!-- 1.2440e-27 (5.71e-01): par = (100 0.1 0.51) -->
<!-- Error in nls(lform, data = ldata, start = lstart2, trace = TRUE) :  -->
<!--   number of iterations exceeded maximum of 50 -->
<!-- > # Fix with scaleOffset -->
<!-- > nls(lform, data = ldata, start = lstart2, trace = TRUE, control = list(scaleOffset = 1.0)) -->
<!-- 40346.      (1.08e+00): par = (100 10 1) -->
<!-- 11622.      (2.91e+00): par = (101.47 0.49449 0.71685) -->
<!-- 5638.0      (9.23e+00): par = (102.23 0.38062 0.52792) -->
<!-- 642.08      (5.17e+00): par = (102.16 0.22422 0.41935) -->
<!-- 97.712      (2.31e+00): par = (100.7 0.14774 0.45239) -->
<!-- 22.250      (1.11e+00): par = (99.803 0.093868 0.50492) -->
<!-- 0.025789    (3.79e-02): par = (100.01 0.10017 0.50916) -->
<!-- 6.0571e-08  (5.80e-05): par = (100 0.1 0.51) -->
<!-- 4.7017e-19  (1.62e-10): par = (100 0.1 0.51) -->
<!-- Nonlinear regression model -->
<!--   model: y ~ a/(1 + b * exp(-c * t)) -->
<!--    data: ldata -->
<!--      a      b      c  -->
<!-- 100.00   0.10   0.51  -->
<!--  residual sum-of-squares: 4.7e-19 -->
<!-- Number of iterations to convergence: 8  -->
<!-- Achieved convergence tolerance: 1.62e-10 -->
<!-- ``` -->

### More general termination tests

The convergence criterion above, the principal termination control of `nls()`, leaves out 
some possibilities that could
be useful for some problems. The package `nlsr` (@nlsr-manual) already offers both 
the safeguarded relative offset test (**roffset**) as well as a **small sum of 
squares** test (**smallsstest**) that compares the latest evaluated sum of squared
(weighted) residuals to `e4` times the initial sum of squares, where
`e4 <- (100*.Machine$double.eps)^4` is approximately 2.43e-55.

We note that `nls()` uses a termination test to stop after `maxiter` "iterations". 
Unfortunately, the meaning of "iteration" varies among programs and requires 
careful examination of the
code. We prefer to use the number of times the residuals or the jacobian have
been computed and put upper limits on these. Our codes exit (terminate) when these limits
are reached. Generally we prefer larger limits than the default `maxiter = 50` of `nls()`,
but that may reflect the more difficult problems we have encountered because 
users consult us when standard tools have given unsatisfactory results. 

## Feature: Failure when Jacobian is computationally singular

This refers to the infamous "singular gradient" termination message of `nls()`. 
A Google search of 

```
R nls "singular gradient"
```

gets over 4000 hits that are spread over some years. This could be because the 
Jacobian is poorly approximated (see **Jacobian computation** below). However,
it is common in nonlinear least squares computations that the Jacobian is very close
to singular for some values of the model parameters. In such cases, we need to find
an alternative algorithm to the Gauss-Newton iteration of `nls()`. The most common
work-around is the Levenberg-Marquardt stabilization (see @Marquardt1963, @Levenberg1944, 
@jn77ima), and versions of it have been implemented in packages `minpack.lm` and `nlsr`.
In our work, we prepared experimental prototypes of `nls` that incorporate
stabilization, but integration with all the features of `nls()` has proved difficult.

Let us consider a 'singular gradient' example 
*https://stats.stackexchange.com/questions/13053/singular-gradient-error-in-nls-with-correct-starting-values*.
`nlsr::nlxb()` displays the singular values of the Jacobian, which confirm that the `nls()` error message is correct,
but this is not helpful in obtaining a solution.

```{r SESingGrad}
reala <- -3; realb <- 5; realc <- 0.5; realr <- 0.7; realm <- 1 # underlying parameters
x <- 1:11 # x values; 11 data points
y <- reala + realb * realr^(x - realm) + realc * x # linear + exponential function
testdat <- data.frame(x, y) # save in a data frame
strt <- list(a = -3, b = 5, c = 0.5, r = 0.7, m = 1) # give programs a good start
jform <- y ~ a + b * r^(x - m) + c * x # Formula
library(nlsr)
linexp2 <- try(nlxb(jform, data = testdat, start = strt, trace = F))
linexp2 # Note singular values of Jacobian in rightmost column
# We get an error with nls()
linexp <- try(nls(jform, data = testdat, start = strt, trace = F))
```

The Jacobian is essentially singular as shown by its singular
values. Note that these are **displayed** by package `nlsr` in a single column in
the output to provide a compact layout, but the values do **NOT** correspond to 
the individual parameters in 
whose row they appear; they are a property of the whole problem.

## Feature: Jacobian computation

`nls()`, with the `numericDeriv()` function, computes the Jacobian as the "gradient"
attribute of the residual vector. This is implemented as a mix of R and C code, but
we have created a rather more compact version entirely in R. 
See https://github.com/nashjc/RNonlinearLS/DerivsNLS .
<!-- Code project. See the companion document **Jacobian Calculations for nls()** -->
<!-- (?? need citation -- currently **DerivsNLS.pdf**). -->

As far as we can understand the logic in `nls()`, the Jacobian computation during
parameter estimation is carried out entirely within the called C-language program,
and the R code `function numericDeriv()`, part of `./src/library/stats/R/nls.R`
in the R distribution source code, is used to provide Jacobian information in 
the `nlsModel()` and `nlsModel.plinear()` functions.

The Jacobian used by `nlsr::nlxb()` is, by default, computed from analytic expressions, while that
of `nls()` mostly uses a numerical approximation. Some selfStart models do provide
"gradient" information. `minpack.lm::nlsLM()` invokes `numericDeriv()` in its local 
version of `nlsModel()`, but it appears to use an internal approximate Jacobian code from the 
original Fortran `minpack` code, namely, `lmdif.f`. Such differences in approach
can lead to different behaviour, though in our experience, one has to look for the
differences, as they are generally minor.

- A pasture regrowth problem (@Huet2004, page 1, based on @Ratkowsky1983) has a poorly
  conditioned Jacobian and `nls()` fails with "singular gradient". 
  Worse, numerical approximation to the Jacobian give the error
  "singular gradient matrix at initial parameter estimates" for `minpack.lm::nlsLM`
  so that the Marquardt stabilization is unable to take effect, while the analytic
  derivatives of `nlsr::nlxb` give a solution. 

- Karl Schilling (private communication) provided an example where a model specified
  with the formula `y ~ a * (x Ë† b)` causes `nlsr::nlxb` to fail because the partial
  derivative w.r.t. `b` is `a * (x^b * log(x))`. If there is data for which `x = 0`,
  this is undefined. In such cases, we observed that `nls()` and `minpack.lm::nlsLM`
  found a solution, though it can be debated whether such lucky accidents can be
  taken as an advantage.

Note that the selfStart models in the base R `./src/library/stats/R/zzModels.R` file may
provide the Jacobian in the "gradient" attribute of the "one-sided" formula that defines each
model, and these Jacobians may be the analytic forms. The `nls()` function, after computing the
"right hand side" or `rhs` of the residual, then checks to see if the "gradient" attribute is
defined, and, if not, uses `numericDeriv` to compute a Jacobian into that attribute. This code
is within the `nlsModel()` or `nlsModel.plinear()` functions. The use of analytic Jacobians
almost certainly contributes to the good performance of `nls()` on selfStart models.

## Feature: Weights on observations

?? does resid return weighted or unweighted residuals

?? do we want unweighted (raw) residuals? or are predictions / fits just as good?

## Feature: Subsetting

`nls()` accepts an argument `subset`. This acts through the mediation of
`model.frame` and is not clearly obvious in the source code files `/src/library/stats/R/nls.R` and 
`/src/library/stats/src/nls.C`. 

While the implementation of subset at the level of the call to `nls()` has a certain
attractiveness, it does mean that the programmer of the solver needs to be aware of the
source (and value) of objects such as the data, residuals and Jacobian. By preference, 
we would implement subsetting by means of zero-value weights, with observation counts
(and degrees of freedom) computed via the numbers of non-zero weights. Alternatively,
we would extract a working dataframe from the relevant elements in the original.

## Feature: na.action

`na.action` is an argument to the `nls()` function, but it does not appear obviously in the
source code, often being handled behind the scenes after referencing the option `na.action`.
A useful, but possibly dated, description is given in:
https://stats.idre.ucla.edu/r/faq/how-does-r-handle-missing-values/. 

The typical default action, which can be seen by using the command `getOption("na.action")`
is `na.omit`. This option essentially omits from computations any observations
containing missing values (i.e. any row of a data frame containing an NA). 
`na.exclude` does much of the same for computations, but keeps the rows with NA elements so 
that predictions are in the correct row position. We recommend that workers actually test 
output to verify the behaviour is as wanted.

A succinct description of the issue is given in: *https://stats.stackexchange.com/questions/492955/should-i-use-na-omit-or-na-exclude-in-a-linear-model-in-r*

> The only benefit of `na.exclude` over `na.omit` is that the former will retain the 
original number of rows in the data. This may be useful where you need to retain the 
original size of the dataset - for example it is useful when you want to compare 
predicted values to original values. With `na.omit` you will end up with fewer 
rows so you won't as easily be able to compare.

`na.pass` simply passes on data "as is", while `na.fail` will essentially stop if any missing
values are present.

Our concern with `na.action` is that users may be unaware of the effects of an option
setting they may not even be aware has been set. Is `na.fail` a safer default?

## Feature: model frame

`model` is an argument to the `nls()` function, which is documented as:

> **model** logical. If true, the model frame is returned as part of the object. Default is FALSE.

Indeed, the argument only gets used when `nls()` is about to return its result object, and the
element `model` is NULL unless the calling argument `model` is TRUE. (Using the same name could
be confusing.) However, the model frame is used within the function code in the form of the object
`mf`. We feel that users could benefit from more extensive documentation and examples of its use.

## Feature: Sources of data

`nls()` can be called without specifying the `data` argument. In this case, it will
search in the available environments (i.e., workspaces) for suitable data objects. 
We do NOT like this approach, but it is "the R way". R allows users to leave many 
objects in the default (.GlobalEnv) workspace. Moreover, users have to actively 
suppress saving this workspace (`.RData`) on exit; otherwise, any such file in the path, 
when R is launched, will be loaded. The overwhelming proportion of R users in our
acquaintance avoid saving the workspace because of the danger of lurking data and
functions which may cause unwanted results.

Nevertheless, to provide compatible behaviour with `nls()`, competing programs 
need to ensure equivalent behaviour, but users should test that the 
operation is as they intend. 

## Feature: missing start vector and self-starting models

Nonlinear estimation algorithms are almost all iterative and need a set of starting
parameters. `nls()` offers a special class of modeling functions called **selfStart** 
models. There are a number of these in base R (`./src/library/stats/R/zzModels.R`)
and others in R packages such as CRAN package `nlraa` (@MiguezNLRAA2021), as well as 
the now-archived package `NRAIA`. Unfortunately, some **selfStart** codes
entangle the calculation of starting values for parameters
with the particulars of the `nls()` code. Though there is a `getInitial()` function,
this is not easy to use to simply compute the initial parameter estimates outside of
`nls()`, in part 
because it may call that function. Such circular references are, in our view, dangerous.
Moreover, we believe that it would be helpful to have selfStart models that allow
users to explicitly provide the starting parameters.

Other concerns are revealed by the example below. Here, the `SSlogis` selfStart function 
is used to generate a set
of initial parameters for a 3-parameter logistic curve. The form used by `SSlogis`
is $y \,\sim\, Asym\,/\, (1 \,+\, exp((xmid\,-\,tt)\,/\,scal))$, but we show how the starting parameters
for this model can be transformed to those of 
another form of the model, namely, $y \,\sim\, b1\,/\,(1 \,+\, b2\,*\,exp(-b3\,*\,t))$.

The code for `SSlogis()` is in `./src/library/stats/R/zzModels.R`.
<!-- ``` -->
<!-- SSlogis <- selfStart(~ Asym/(1 + exp((xmid - input)/scal)), -->
<!--     selfStart( -->
<!--         function(input, Asym, xmid, scal) -->
<!--         { -->
<!--               .expr1 <- xmid - input -->
<!--               .expr3 <- exp(.e2 <- .expr1/scal) -->
<!--               .expr4 <- 1 + .expr3 -->
<!--               .value <- Asym/.expr4 -->
<!--               .actualArgs <- as.list(match.call()[c("Asym", "xmid", "scal")]) -->
<!--               if(all(vapply(.actualArgs, is.name, NA))) -->
<!--               { -->
<!--             		  .expr10 <- .expr4^2 -->
<!--                   .grad <- array(0, c(length(.value), 3L), list(NULL, c("Asym", "xmid", "scal"))) -->
<!--                   .grad[, "Asym"] <- 1/.expr4 -->
<!-- 		              .grad[, "xmid"] <- - (xm <- Asym * .expr3/scal/.expr10) -->
<!-- 		              .grad[, "scal"] <- xm * .e2 -->
<!--                   dimnames(.grad) <- list(NULL, .actualArgs) -->
<!--                   attr(.value, "gradient") <- .grad -->
<!--               } -->
<!--               .value -->
<!--         }, -->
<!--         initial = function(mCall, data, LHS, ...) { -->
<!--               xy <- sortedXyData(mCall[["input"]], LHS, data) -->
<!--               if(nrow(xy) < 4) { -->
<!--                   stop("too few distinct input values to fit a logistic model") -->
<!--               } -->
<!--               z <- xy[["y"]] -->
<!--               ## transform to proportion, i.e. in (0,1) : -->
<!--               rng <- range(z); dz <- diff(rng) -->
<!--               z <- (z - rng[1L] + 0.05 * dz)/(1.1 * dz) -->
<!--               xy[["z"]] <- log(z/(1 - z))		# logit transformation -->
<!--               aux <- coef(lm(x ~ z, xy)) -->
<!--               pars <- coef(nls(y ~ 1/(1 + exp((xmid - x)/scal)), -->
<!--                                data = xy, -->
<!--                                start = list(xmid = aux[[1L]], scal = aux[[2L]]), -->
<!--                                algorithm = "plinear", ...)) -->
<!--               setNames(pars [c(".lin", "xmid", "scal")], -->
<!--                        mCall[c("Asym", "xmid", "scal")]) -->
<!--         }, -->
<!--         parameters = c("Asym", "xmid", "scal")) -->
<!-- ``` -->
This R function includes analytic expressions for the Jacobian ("gradient").
These could be useful to R users, especially if documented. Moreover, we
wonder why the programmers have chosen to save so many quantities in "hidden"
variables, i.e., with names preceded by ".". These are then not displayed by the `ls()`
command, making them difficult to access.

In the event that a selfStart model is not available, `nls()` sets all the starting parameters 
to 1. This is, in our view, tolerable, but could be improved by using a set of values
that are all slightly different, which, in the case of the example model $y \,\sim\, a \,*\, exp(-b \,*\, x) + c\,*\,exp(-d \,*\, x)$
would avoid a singular Jacobian because $b$ and $d$ were equal in value. 
A sequence like  1.0, 1.1, 1.2, 1.3 for the four parameters 
could be provided quite simply instead of all 1's.

<!-- ```{r hobbsSSlogis} -->
<!-- weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, -->
<!--           38.558, 50.156, 62.948, 75.995, 91.972) -->
<!-- tt <- 1:12 -->

<!-- NLSformula0 <- y ~ b1/(1+b2*exp(-b3*tt)) -->
<!-- NLSformula <- y ~ SSlogis(tt, Asym, xmid, scal) -->
<!-- NLSformulax <- y ~ Asym/(1+exp((xmid-tt)/scal)) -->
<!-- NLStestdata <- data.frame(y=weed, tt=tt)  -->
<!-- s0 <- getInitial(NLSformula, NLStestdata) -->
<!-- print(s0) -->
<!-- # We transform the parameters for the NLSformula0 model of original specification. -->
<!-- s1<-list(b1=s0[1], b2=exp(s0[2]/s0[3]), b3=1/s0[3]) -->
<!-- print(as.numeric(s1)) -->
<!-- # No actual improvement because nls() has been already used to get the starting values, -->
<!-- # but we do get SEs -->
<!-- hobblog<-nls(NLSformula0, data=NLStestdata, start=s1) -->
<!-- summary(hobblog) -->
<!-- deviance(hobblog) -->
<!-- # nls fails without selfStart -- singular gradient, even on stabilized formula -->
<!-- try(hobblogx<-nls(NLSformulax, data=NLStestdata)) -->
<!-- # But Marquardt is able to get a solution easily -->
<!-- library(nlsr) -->
<!-- hobblogxx<-nlxb(NLSformulax, data=NLStestdata, start=c(Asym=1, xmid=1, scal=1)) -->
<!-- hobblogxx -->
<!-- ``` -->

<!-- ### selfStart models in base R -->

<!-- The following models are provided (in file ./src/library/stats/R/zzModels.R) -->

<!-- ``` -->
<!-- SSasymp         - asymptotic regression model -->
<!-- SSasympOff      - alternate formulation of asymptotic regression model with offset -->
<!-- SSasympOrig     - exponential curve through the origin to an asymptote -->
<!-- SSbiexp         - y ~ ~ A1 * exp(-exp(lrc1)*input) + A2 * exp(-exp(lrc2) * input) -->
<!-- SSfol           - y ~ Dose * (exp(lKe + lKa - lCl) * (exp(-exp(lKe) * input) - -->
<!--                           exp(-exp(lKa) * input))/(exp(lKa) - exp(lKe))) -->
<!-- SSfpl           - four parameter logistic model -->
<!-- SSlogis         - three parameter logistic model -->
<!-- SSmicmen        - Michaelis-Menten model for enzyme kinetics -->
<!-- SSgompertz2     - Gompertz model for growth curve data -->
<!-- SSweibull       - Weibull model for growth curve data -->
<!-- ``` -->

### Strategic issues in selfStart models

Because the Gauss-Newton algorithm is unreliable from many sets of starting
of parameters, selfStart models are a part of the `nls()` infrastructure rather
than an accessory. However, creating such functions is a lot of work, and their
documentation (file `./src/library/stats/man/selfStart.Rd`) is quite complicated.
We believe that the focus should be placed on getting good initial parameters,
that is `getInitial()` function, though avoiding the current calls back to `nls()`.
Interactive tools, such as "visual fitting" (@nash1996nonlinear) might be
worth considering.

We also note that the introduction of `scaleOffset` in R 4.1.1 to deal with the 
convergence test for small residual problems now requires that the `getInitial()`
function have dot-arguments (`...`) in its argument list. This illustrates the
entanglement of many features in `nls()`.


## Issue: returned output of nls() and its documentation

The output of `nls()` is an object of class "nls" which has the following structure:

>A list of

>`m` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  an `nlsModel` object incorporating the model.

>`data`	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;the expression that was passed to `nls` as the data argument. The actual \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$  data values are present in the environment of the `m` components, e.g., \newline $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ `environment(m$conv)`.

>`call` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the matched call with several components, notably `algorithm`.

>`na.action` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the `"na.action"` attribute (if any) of the model frame.

>`dataClasses` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $~$the `"dataClasses"` attribute (if any) of the "`terms`" attribute of the \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ model frame.

>`model` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;if `model = TRUE`, the model frame.

>`weights` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if `weights` is supplied, the weights.

>`convInfo` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; a list with convergence information.

>`control` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the control `list` used, see the `control` argument.

>`convergence, message` &nbsp;for an `algorithm = "port"` fit only, a convergence code (`0` for  \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$convergence) and message.

> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; To use these is *deprecated*, as they are available from `convInfo` now.

                  -----------------------------------------------------------

<!-- ### Example output -->

<!-- To illustrate, let us run the Croucher example. -->

<!-- ```{r nlsoutx} -->
<!-- # Croucher example -->
<!-- xdata <- c(-2,-1.64,-1.33,-0.7,0,0.45,1.2,1.64,2.32,2.9) -->
<!-- ydata <- c(0.699369,0.700462,0.695354,1.03905,1.97389,2.41143,1.91091,0.919576,-0.730975,-1.42001) -->
<!-- p1<- 1; p2<-0.2; NLSstart<-list(p1=p1,p2=p2) -->
<!-- NLSformula <- ydata ~ p1*cos(p2*xdata) + p2*sin(p1*xdata) -->
<!-- NLSdata<-data.frame(xdata, ydata) -->
<!-- # Try full output version of nls -->
<!-- library(nlspkg) # use the packaged version of nls() -->
<!-- result<-nls(NLSformula, data=NLSdata, start=NLSstart, model=TRUE) -->
<!-- # We can display the result with several commands -->
<!-- # str(result) -- displays large amount of material - suppressed here -->
<!-- #    as it is too wide for the page -->
<!-- # result # displays the object -->
<!-- # ls(result) # to list the elements of the output -->
<!-- # ls(result$m) # and in particular the "m" object -->
<!-- ``` -->

<!-- ### Concerns with content of the nls result object -->

The `nls` object contains some elements that are awkward to produce by other algorithms,
but some information that would be useful is not presented in a clear manner. Moreover, the
complexity of the object is a challenge to users.

In the following, we use `result` as the returned object from `nls()`.

The `data` return element is an R symbol. To actually access the data from this 
element, we need to use the syntax:

```
eval(parse(text=result$data))
```

However, if the call is made with `model=TRUE`, then there is a returned element
`model` which contains the data, and we can list its contents using:

```
ls(result$model)
```

and if there is an element called `xdata`, it can be accessed as `result$model$xdata`.

Let us compare the `nls()` result with that from 
`nlsr::nlxb()`, with ostensibly solves the same problem:


>`coefficients`    A named vector giving the parameter values at the supposed solution.

>`ssquares` $~~~~~$ The sum of squared residuals at this set of parameters.

>`resid` $~~~~~~~~~~$ The residual vector at the returned parameters.

>`jacobian` $~~~~~$ The jacobian matrix (partial derivatives of residuals w.r.t. the 
                parameters) at the \newline
$~~~~~~~~~~~~~~~~~~~~$ returned parameters.

>`feval` $~~~~~~~~~~$ The number of residual evaluations (sum of squares computations) used.

>`jeval` $~~~~~~~~~~$ The number of Jacobian evaluations used.


However, actually looking at the structure of a returned result gives a list of 11
items. The extra 5 are:

```
 $ lower       : num [1:3] -Inf -Inf -Inf
 $ upper       : num [1:3] Inf Inf Inf
 $ maskidx     : int(0) 
 $ weights     : NULL
 $ formula     :Class 'formula'  language y ~ Asym/(1 + exp((xmid - tt)/scal))
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
 - attr(*, "class")= chr "nlsr"
```
The result object from `nlsr::nlxb()` is still much smaller than the one `nls()` returns. 
Moreover, `nlxb`
explicitly returns the sum of squares as well as the residual vector and Jacobian. 
The counts of evaluations are also returned.
(Note that the singular values of the Jacobian are
actually computed in the `print` and `summary` methods for the result.) 
We noted several potential updates to the `nlsr` documentation as well as that
for `nls()` as we made our comparisons.

<!-- nlsr2 has some extra info ?? -->

### Weights in returned functions from nls()

The functions `resid()` (an alias for `residuals()`) and `fitted()` and `lhs()` are UNWEIGHTED.
But if we return `ans` from `nls()` or `minpack.lm::nlsLM` or our new `nlsj` (interim package), 
then `ans$m$resid()` is WEIGHTED. This needs clear documentation so users can get the answers
intended.

SHORT EXAMPLE??

```{r nlswtx, echo=TRUE}
weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972)
tt <- 1:12
weeddf <- data.frame(tt, weed)
wts <- 0.5^tt # simple weights
frmlogis <- weed ~ Asym/(1 + exp((xmid - tt)/scal))
Asym<-1; xmid<-1; scal<-1
nowt<-nls(weed ~ SSlogis(tt, Asym, xmid, scal))
nowt
nowt$m$resid()
inpar<-getInitial(weed ~ SSlogis(tt, Asym, xmid, scal), data=weeddf)
print(inpar)
library(nlsr2)
nowtx<-nlxb(frmlogis, start=c(Asym=1, xmid=1, scal=1), trace=TRUE)
nowtx
# nowtSSx0<-nlxb(weed ~ SSlogis(tt, Asym=1, xmid=1, scal=1), start=c(Asym=1, xmid=1, scal=1), trace=TRUE)
# nowtSsx0
# nowtSSx<-nlxb(weed ~ SSlogis(tt, Asym=1, xmid=1, scal=1), start=c(Asym=100, xmid=2, scal=1), trace=TRUE, 
#               control=list(japprox="jand"))
# nowtSSx
nowtxi<-nlxb(frmlogis, start=inpar, trace=TRUE)
nowtxi
usewt <- nls(weed ~ SSlogis(tt, Asym, xmid, scal), weights=wts)
usewt
usewt$m$resid()
nmod <- nlsModel(frmlogis, data=weeddf, start=c(Asym=1, xmid=1, scal=1), wts=wts)
usewtx<-nlxb(frmlogis,  start=c(Asym=1, xmid=1, scal=1), weights=wts, trace=TRUE)
usewtx
# NOTE jafwd and jaback fail!
usewtxn<-nlxb(frmlogis,  start=c(Asym=1, xmid=1, scal=1), weights=wts, trace=TRUE, control=list(japprox="jacentral"))
usewtxn
usewtxi<-nlxb(frmlogis, start=inpar, weights=wts, trace=TRUE)
usewtxi
# usewtx$resid
```

### Interim output from the "port" algorithm

As the `nls()` **man** page states, when the "port" algorithm is used with the `trace` argument
TRUE, the iterations display the objective function value which is 1/2 the sum of squares (or 
deviance). It is likely that the trace display is embedded in the Fortran of the `nlminb`
routine that is called to execute the "port" algorithm, but the discrepancy is nonetheless
unfortunate for users. 

### Failure to return best result achieved

If `nls()` reaches a point where it cannot continue but has not found a point where the
relative offset convergence criterion is met, it may simply exit, especially if a 
"singular gradient" (singular Jacobian) is found. However, this may occur AFTER the 
function has made considerable progress in reducing the sum of squared residuals. 
<!-- An example is to be found in the `Tetra_1.R` example from the `nlsCompare` package. -->
<!-- ?? AB: Are we far enough along to include reference? -->
Here is an abbreviated example:

```{r tetrarun}
time <- c( 1,  2,  3,  4,  6 , 8, 10, 12, 16)
conc <- c( 0.7, 1.2, 1.4, 1.4, 1.1, 0.8, 0.6, 0.5, 0.3)
NLSdata <- data.frame(time,conc)
NLSstart <- c(lrc1 = -2, lrc2 = 0.25, A1 = 150, A2 = 50) # a starting vector (named!)
NLSformula <- conc ~ A1 * exp(-exp(lrc1) * time) + A2 * exp(-exp(lrc2) * time)
tryit <- try(nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE))
print(tryit)
```

Note that the sum of squares has been reduced from 61216 to 1.6211, but 
unless `trace` is invoked, the user will not get any information about this.
This almost trivial change to the `nls()` function could be 
useful to R users.

## Feature: partially linear models and their specification

Specifying a model to a solver should, ideally, use the same syntax across
solver tools. Unfortunately, R allows multiple approaches within different
modelling tools, and within `nls()` itself.

The nonlinear modeling model specifications are, of course, a development of linear ones.
Unfortunately, the explicit model `y ~ a * x + b` does not work with the linear modeling 
function `lm()`, which requires this model to be specified as `y ~ x`.

Within `nls()`, consider the following FOUR different specifications for the same problem, plus 
an intuitive choice, labelled fm2a, that does not work. In this failed
attempt, putting the `Asym` parameter in the model causes the `plinear` algorithm
to try to add another term to the model. We believe this is unfortunate, and would
like to see a consistent syntax. At the time of writing, we do
not foresee a resolution for this issue. In the example, we have NOT evaluated
the commands to save space.

```{r log4ways, echo=TRUE, eval=FALSE}
DNase1 <- subset(DNase, Run == 1) # select the data
## using a selfStart model - do not specify the starting parameters
fm1 <- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
summary(fm1)

## using conditional linearity - leave out the Asym parameter
fm2 <- nls(density ~ 1 / (1 + exp((xmid - log(conc)) / scal)),
                 data = DNase1, start = list(xmid = 0, scal = 1),
                 algorithm = "plinear")
summary(fm2)

## without conditional linearity
fm3 <- nls(density ~ Asym / (1 + exp((xmid - log(conc)) / scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1))
summary(fm3)

## using Port's nl2sol algorithm
fm4 <- try(nls(density ~ Asym / (1 + exp((xmid - log(conc)) / scal)),
                 data = DNase1, start = list(Asym = 3, xmid = 0, scal = 1),
                 algorithm = "port"))
summary(fm4)

## using conditional linearity AND Asym does NOT work
fm2a <- try(nls(density ~ Asym / (1 + exp((xmid - log(conc)) / scal)), 
                 data = DNase1, start = list(Asym=3, xmid = 0, scal = 1),
                 algorithm = "plinear", trace = TRUE))
summary(fm2a)
```

## Issue: code structure and documentation for maintenance 

The `nls()` code is structured in a way that inhibits both maintenance and improvement.
In particular, the iterative setup is such that introduction of Marquardt stabilization
is not easily available. To obtain performance when the code was written, a large portion
was written in C with consequent calls and returns that
severely complicate the code. Over time, R has become much more efficient on modern computers, and
the need to use compiled C and Fortran is less critical. Moreover, the burden for maintenance
could be reduced by moving code entirely to R. 

While R and its packages are generally very well documented for usage, that documentation
rarely extends as far as it might for code maintenance. 
The paucity of such documentation is exacerbated by the mixed R/C/Fortran code base,
where seemingly trivial differences in things like representation of numbers or
base index for arrays lead to traps for unwary programmers.

<!-- For `nls()` the concern is demonstrated by a short email query from John Nash -->
<!-- to Doug Bates and his reply. -->
<!-- This is NOT a criticism of Prof. Bates' (or any other) work, but a reflection on  -->
<!-- how difficult it is to develop code in this subject area and to keep it maintainable.  -->
<!-- We have experienced similar loss of understanding for some of our own codes. -->

<!-- ``` -->
<!--     ... -->
<!--     https://gitlab.com/nashjc/improvenls/-/blob/master/Croucher-expandednlsnoc.R -->
<!--     ... has the test problem and the expanded code. Around line 367 is where we are -->
<!--     scratching our heads. The function code (from nlsModel()) is in the commented  -->
<!--     lines below the call. This is -->

<!--           # > setPars -->
<!--           # function(newPars) { -->
<!--           #   setPars(newPars) -->
<!--           #   resid <<- .swts * (lhs - (rhs <<- getRHS())) # envir = thisEnv {2 x} -->
<!--           #   dev   <<- sum(resid^2) # envir = thisEnv -->
<!--           #   if(length(gr <- attr(rhs, "gradient")) == 1L) gr <- c(gr) -->
<!--           #   QR <<- qr(.swts * gr) # envir = thisEnv -->
<!--           #   (QR$rank < min(dim(QR$qr))) # to catch the singular gradient matrix -->
<!--           # } -->

<!--     I'm anticipating that we will be able to set up a (possibly inefficient) code -->
<!--     with documentation that will be easier to follow and test, then gradually figure -->
<!--     out how to make it more efficient. -->

<!--     The equivalent from minpack.lm is -->

<!--     setPars = function(newPars) { -->
<!--                 setPars(newPars) -->
<!--                 assign("resid", .swts * (lhs - assign("rhs", getRHS(), -->
<!--                     envir = thisEnv)), envir = thisEnv) -->
<!--                 assign("dev", sum(resid^2), envir = thisEnv) -->
<!--                 assign("QR", qr(.swts * attr(rhs, "gradient")), envir = thisEnv) -->
<!--                 return(QR$rank < min(dim(QR$qr))) -->
<!--             } -->

<!--     In both there is the recursive call, which must have a purpose I don't understand. -->
<!-- ``` -->
<!-- The reply was -->

<!-- ``` -->
<!--     I'm afraid that I don't know the purpose of the recursive call either.   -->
<!--     I know that I wrote the code to use a closure for the response, covariates, etc.,  -->
<!--     but I don't recall anything like a recursive call being necessary. -->
<!--     ... -->
<!-- ``` -->
<!-- <!--     I don't think I will be of much help.  My R skills have atrophied to the point  --> -->
<!-- <!--     where I wouldn't even know how to start exploring what is happening in the first  --> -->
<!-- <!--     call as opposed to the recursive call. --> -->
<!-- <!-- ``` --> -->


## Feature: indexed parameters

The **man** file for `nls()` includes the following example of a situation in which
parameters are indexed. It also uses the "plinear" option as an added complication.

<!-- Here we use a truncated version of the example to save display space. -->

<!-- ```{r nlsindx1} -->
<!-- ## The muscle dataset in MASS is from an experiment on muscle -->
<!-- ## contraction on 21 animals.  The observed variables are Strip -->
<!-- ## (identifier of muscle), Conc (Cacl concentration) and Length -->
<!-- ## (resulting length of muscle section). -->
<!-- if(! requireNamespace("MASS", quietly = TRUE)) stop("Need MASS pkg") -->
<!-- mm<- MASS::muscle[1:12,] # take only 1st few values of Strip -->
<!-- str(mm) -->
<!-- mm<-droplevels(mm) -->
<!-- str(mm) -->
<!-- nlev <- nlevels(mm) -->
<!-- withAutoprint({ -->
<!--   ## The non linear model considered is -->
<!--   ##       Length = alpha + beta*exp(-Conc/theta) + error -->
<!--   ## where theta is constant but alpha and beta may vary with Strip. -->
<!--   with(mm, table(Strip)) # 2, 3 or 4 obs per strip -->
<!--   nl <- nlevels(mm$Strip) -->
<!--   ## We first use the plinear algorithm to fit an overall model, -->
<!--   ## ignoring that alpha and beta might vary with Strip. -->
<!--   musc.1 <- nls(Length ~ cbind(1, exp(-Conc/th)), mm, -->
<!--                 start = list(th = 1), algorithm = "plinear") -->
<!--   summary(musc.1) -->

<!--   ## Then we use nls' indexing feature for parameters in non-linear -->
<!--   ## models to use the conventional algorithm to fit a model in which -->
<!--   ## alpha and beta vary with Strip.  The starting values are provided -->
<!--   ## by the previously fitted model. -->
<!--   ## Note that with indexed parameters, the starting values must be -->
<!--   ## given in a list (with names): -->
<!--   ## ?? but why use b here AND in the new formula?? -->
<!--   b <- coef(musc.1) -->

<!--   musc.2 <- nls(Length ~ a[Strip] + b[Strip]*exp(-Conc/th), data=mm, -->
<!--                 start = list(a = rep(b[2], nl), b = rep(b[3], nl), th = b[1])) -->
<!--   summary(musc.2) -->
<!-- }) -->

<!-- ``` -->

Running the example reveals that the answers for the parameters are NOT indexed. 
<!-- e.g. `coef(musc.2)` is a single level vector of parameters.  -->
We do not see `a[1], a[2], a[3]` but `a1, a2, a3`. 
<!-- This is because the model -->
<!-- must integrate all the parameters because `th` is a common parameter across the index `Strip`. -->
This is no doubt because programming for indexed parameters is extremely challenging.
However, we believe the current structure is could cause confusion and error, and 
propose an alternative approach below.

## Feature: bounds on parameters

There are many situations where the context of a problem constrains the values of parameters.
For example, one of us was asked many years ago to estimate a model where one parameter estimate
was negative. The client complained "But that is supposed to be the number of grain elevators
in Saskatchewan". The number should have been a positive integer, and likely less
than a few thousand.

`nls()` can impose bounds on parameters, but only if the `port` algorithm is used. Unfortunately,
the manual states 

> The `algorithm = "port"` code appears unfinished, and does not even check that the starting 
  value is within the bounds. Use with caution, especially where bounds are supplied.

This is clearly unsatisfactory for general use, but does find bounded solutions. 
Inclusion of bounds on parameters in nonlinear least squares computations is relatively 
straightforward, and it is part of the default method
for package `nlsr`. Package `minpack.lm` also includes provision for bounds, but the following
script, which we do not run here for reasons of space, shows that it does not find a
good solution for the scaled Hobbs test problem.

```{r chkbound}
# bhobbsX.R ## bounded formula specification of problem using Hobbs Weed problem
weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972)
tt <- 1:12
wf <- data.frame(y = weed, tt = tt)
st <- c(b1 = 1, b2 = 1, b3 = 1) # a default starting vector (named!)
wmods <- y ~ 100 * b1 / (1 + 10 * b2 * exp(-0.1 * b3 * tt)) ## Scaled model
require(minpack.lm)
require(nlsr)
# Hobbs scaled problem with bounds, formula specification
anlxb1 <- nlxb(wmods, start = st, data = wf, lower = c(0, 0, 0), upper = c(2, 6, 3))
cat("Using nlsr::nlxb():")
print(anlxb1)
## nlsLM seems NOT to work properly with bounds
anlsLM1b <- nlsLM(wmods, start = st, data = wf, lower = c(0, 0, 0), upper = c(2, 6, 3))
cat("\n"); cat("using minpack.lm::nlsLM():")
print(anlsLM1b)
anlsb<-nls(wmods, start = st, data = wf, algorithm = "port", lower = c(0, 0, 0), upper = c(2, 6, 3))
cat("Using nls with 'port':")
print(anlsb)
```

### Philosophical considerations

Bounds on parameters raise some interesting and difficult questions about how uncertainty
in parameter estimates should be computed or reported. That is, the traditional "standard
errors" are generally taken to imply symmetric intervals about the point estimate in which
the parameter may be expected to be found with some probability under certain assumptions.
Bounds change those assumptions. Hence, `nlsr::nlxb()` does not compute standard
errors nor their derived statistics when bounds are active.

# Goals and progress of our effort to improve `nls()`

## Code rationalization and documentation

We want:

- to provide a packaged version of `nls()` (call it `nlsalt`) coded entirely in R 
  that matches the features in base R or what is packaged in `nlspkg` as described 
  in @PkgFromRbase22.
  
- to streamline the overall `nls()` infrastructure. By this we
  mean a re-factoring of the routines so they are better suited to maintenance of both
  the existing `nls()` methods and features as well as new features we or others
  would like to add.

- to explain what we do, either in comments or separate maintainer documentation.
  Since we are complaining about the lack of explanatory material for the current
  code, we feel it is incumbent on us to provide such material for our own work, and if
  possible for the existing code.
  
These goals echo themes in the recent discussion @Vidoni21 and accompanying 
commentary.

### Rationalization of formula specifications

We have seen that
`nls()` uses a different formula specification from the default if the 
`plinear` algorithm is used. This is unfortunate, since the user cannot then simply
add `algorithm="plinear"` to the call, making errors more likely. 

While not consistent with the `lm()` specification of linear models, 
nonlinear models need all parameters specified. 
That is, the model formula should be written down as it would be computed.
Thus a linear model (which is valid input for nonlinear estimatin) should
be specified `y ~ a * x + b` rather than `y ~ x`. 
Moreover, we want to avoid the partially linear parameters appearing 
in the result object as `.lin1`, `.lin2`, etc., where the actual position 
of parameters in the model is not explicit.

A possible workaround to allow consistency would be to specify the 
partially linear parameters when the `plinear` option is specified. For example, 
we could use `algorithm="plinear(Asym)"` while keeping the general model formula 
from the default specification. This would allow 
for the output of different `algorithm` options to be consistent. However, we have not
yet tried to code such a change, and welcome discussion from those working with
other packages using model formulas.

A further complication of model specification in R is that some formulas require
the user to surround a term with `I()` to inhibit interpretation of arithmetic 
operators. We would prefer that formulas be usable without this complication.

<!-- It is also of interest to consider code that will detect partial linearity,  -->
<!-- rather than expecting the user to provide the special structure and settings. Work in this -->
<!-- area appears to be quite limited, with @Birkisson2013 seemingly a sole contribution -->
<!-- we can find. -->
<!-- Unfortunately, the mention of a "Linear and Nonlinear Discoverer" in @Zhang2011LinNonlin  -->
<!-- is about a class of models that have almost nothing in common -->
<!-- with the present issue. The algorithm in @Birkisson2013 will need investigating further, -->
<!-- but we suspect that it is complicated enough that it will not easily be adapted to  -->
<!-- a program like `nls()`. Possibly a simpler algorithm may be found. -->

<!-- ``` -->
<!-- y ~ x + z + x*z -->
<!-- y ~ a + b*x +c*z + d * (x*z)  # <-- fully specified -->
<!-- y ~ a*Fn1(b, c, x, z)/Fn2() -->

<!-- ``` -->

### Rationalization of indexed models

Indexed models clearly have a place in some areas of research. 
<!-- We need to be able to process models, as the example above, where the -->
<!-- model formula is  -->
We need to be able to process models such as 
`Length ~ a[Strip] + b[Strip] * exp(-Conc / th)` where `Strip` is the index. Data
`Length` and `Conc` are available for all observations and parameter `th` applies
to every fitted point, but `a[Strip]` and `b[Strip]` depend on the index, which
could be an experimental run number, or some other categorization. 

Given that `[ ]` are the R method to index arrays, their presence in a model
formula signals indexing. The question is how to perform the correct calculations.

<!-- While tedious (??maybe dummies or fastdummies packages make it less so?),  -->
<!-- we could set up the model as a sum of similar models with dummy -->
<!-- variables to turn "on" or "off" the particular case, transforming the model -->
<!-- parameters to simple, non-indexed ones (as does `nls()`). -->

Our view is that inclusion of the indexing **within** `nls()` introduces 
complexity that could be avoided by a suitable wrapper function. The modeling
formula must be expanded to encompass all the index cases, likely with indexed
parameters renamed e.g., `a[2]` becomes `a2` as at present with `nls()`. However,
we believe the wrapper should be able to process output to present the indexed
parameters correctly. 

### Streamlining code

We are pessimistic that the overall structure of `nls()` can be streamlined due
to the entanglement of so many features with the complicated mix of R, C and
Fortran. Indeed, despite digging into the code over some months, we do not feel
confident that we sufficiently understand it nor that we could maintain it. On the other
hand, we do believe equivalent functionality can be built, but with at least
some differences in how the features will be accessed.

## Tests and use-case examples

Maintainers of packages need suitable tests and use-case examples in order:

- to ensure packages work properly, in particular, giving
  results comparable to or better than the functions they are to replace;
- to test individual solver functions to ensure they work across the range of calling 
  mechanisms, that is, different ways of supplying inputs to the solver(s);
- to pose "silly" inputs to see if 
  these bad inputs are caught by the programs.
  
Such goals align with the aims of **unit testing** 
(e.g., https://towardsdatascience.com/unit-testing-in-r-68ab9cc8d211, @HWtestthat11, @HWdevtools21
and the conventional R package testing tools).
 
<!-- ??AB: perhaps we should put in the refernces?? -->
<!-- I've emailed author to ask if there is preferred citation. JN -->

### A testing package: NLSCompare

One of us has developed a working prototype package at https://github.com/ArkaB-DS/nlsCompare .
A primary design objective of this is to allow the summarisation of multiple tests in
a compact output. The prototype has a vignette to illustrate its use.

<!-- Arkajyoti -- do you want to expand? -->
<!-- No. I think it would be tangential to the goal of the paper. We can have a -->
<!-- separate article for that, maybe? -->
<!-- Trying to trim a lot of words, so am making reference to package. Is it ready -->
<!-- for others to use? -->

<!-- When we have a "new" or trial solver function, we would like to know if it gives  -->
<!-- acceptable results on a range of sample problems of different types, starting -->
<!-- parameters, input conditions, constraints, subsets, weights or other settings.  -->
<!-- Ideally we want to be able to get a summary that is easy to read and assess. For -->
<!-- example, one approach would be to list the names of a set of tests with a red, green -->
<!-- or yellow dot beside the name for FAILURE, SUCCESS, or "NOT APPLICABLE". In the last -->
<!-- category would be a problem with constraints that the solver is not designed to  -->
<!-- handle.  -->

<!-- To accomplish this, we need a suitable "runner" program that can be supplied with -->
<!-- the name of a solver or solvers and a list of test problem cases. Problems generally -->
<!-- have a base setup -- a specification of the function to fit as a formula, some data -->
<!-- and a default starting set of parameters. Other cases can be created by imposing -->
<!-- bounds or mask constraints, subsets of the data, and different starts.  -->

<!-- How to set up this "runner" and its supporting infrastructure is non-trivial. While -->
<!-- the pieces are not as complicated as the inter-related parts of the solvers, especially -->
<!-- `nls()`, the categorization of tests, their documentation, and the structuring to make -->
<!-- running them straightforward and easy requires much attention to detail. -->

<!-- Some considerations for our test scripts: -->

<!-- - Is it useful to have a "base" script for each family of test problem, with numbered particular -->
<!--   cases? That is, if we run the scripts in order, we can avoid some duplication of code and -->
<!--   data. -->

<!-- - While we have developed some tags to document the test problem families and cases, we believe -->
<!--   that such tags (essentially summary documentation) will continue to need revision as different -->
<!--   tools and problems are included in scope of `nlsCompare`. -->

<!-- - Similarly, we expect that there will be ongoing review of the structure of the result files. -->


<!-- # Progress so far -->

<!-- Much of our work is reported in https://github.com/nashjc/RNonlinearLS . -->

<!-- ## Reports and articles -->

<!-- - The present article is the central report of the Google Summer or Code 2021 project -->
<!--   "Improvements to nls()". -->

<!-- - VarietyInNonlinearLeastSquaresCodes.Rmd: a review of the different algorithms and -->
<!--   the many choices in their implementation for nonlinear least squares. This is still -->
<!--   a DRAFT at 2022-6-20. -->

<!-- Response to query about TestsDoc -->
<!-- 4. Testing infrastructure, I think, is a general topic and not -->
<!-- particularly relevant to nls(); i.e., it is important for all packages. -->
<!-- So, I don't think we need to include that in RefactoringNLS. Could be a -->
<!-- separate blog maybe?   -->
  
<!-- Arkajyoti: I think TestsDoc is too preliminary to mention, though the links might be -->
<!-- useful. Perhaps incorporate in the nlsCompare repo. -->
<!-- - TestsDoc.Rmd (?citation) is a survey of testing tools in R. It has more  -->
<!--   general possibilities  -->
<!--   and fits into the subject of regression testing, in which case a  -->
<!--   more extensive literature review will be needed.  Note that this -->
<!--   document reflects the work in the the "Problem sets and test infrastructure" -->
<!--   below. -->

  <!-- need to incorporate MachID and -->
  <!-- the many tests. Arkajyoti: should we think of working this up into a paper for -->
  <!-- JSS or the R-Journal? My  -->
  <!-- view is that this COULD be a long term side-interest for your academic work, but -->
  <!-- that would depend on your own interests as well as opportunities. -->

<!-- This needs more focus of where it may appear. I'm thinking of a blog post -- JN-->
<!-- - DerivsNLS.Rmd ?citation: a document to explain different ways in which Jacobian information -->
<!--   is supplied to nonlinear least squares computation in R. File `ExDerivs.R` is -->
<!--   a DRAFT of a script to provide examples. -->

## Documentation and resources

In our investigation, we built several resources, which are now part of 
the repository https://github.com/nashjc/RNonlinearLS/. Particular items
are:

- A BibTex bibliography for use with all documents in this project, but which 
  has wider application to nonlinear least squares projects in general
  (https://github.com/nashjc/RNonlinearLS/blob/main/BibSupport/ImproveNLS.bib)

- `MachID.R` offers a suggested concise summary function
  to identify a particular computational system used for tests. A discussion 
  of how it was built and the resources needed across platforms is given in
  at https://github.com/nashjc/RNonlinearLS/tree/main/MachineSummary

- @PkgFromRbase22 is an explanation of the construction of the `nlspkg` from the
  `nls()` code in R-base. 

- As the 2021 Summer of Code period was ending, one of us (JN) was invited to prepare
  a review of optimization in R. Ideas from the present work have been instrumental
  in the creation of @NashWires22.

<!-- ?? probably don't need to include -->
<!-- - WorkingDocument4ImproveNLS.Rmd: essentially a way to record what we have worked on. -->
<!--   A project diary. -->

<!-- ?? does not seem to add enough? -->
<!-- ## Problem sets and test infrastructure -->  

<!-- ?? need to clean up -->

<!-- We have several test problems and variants thereof in the `inst/scripts/` directory of -->
<!-- the `nlsCompare` package available on Github (https://github.com/ArkaB-DS/nlsCompare). -->
<!-- We direct the reader to that package for documentation of the test infrastructure,  -->
<!-- in particular the problems and methods files (`problems.csv` and `methods.csv`) and -->
<!-- the various functions invoked by `run.R` to produce an output file in CSV form also. -->

<!-- Towards the end of the project, we have focused our attention on the `nlsCompare` -->
<!-- package, which looks at evaluating and comparing functions for nonlinear least -->
<!-- squares problems. We use many of the same tests as checks that are or will be  -->
<!-- built into our packages for such problems e.g., `nlsalt`. These use the `testthat` -->
<!-- structure and may include verification of outputs that are specific to a package, -->
<!-- such as the upper triangular matrix R of the QR decomposition that has been  -->
<!-- computed for a Jacobian. Since such an object will not be computed by all methods,  -->
<!-- testing it in `nlsCompare` makes no sense, and in that package we concentrate on -->
<!-- the minimum sum of squares and the associated model parameters. -->

<!-- ## Code development -->

<!-- A summary of our main results: -->

<!-- - nlspkg: a packaged version of the `nls() code from R-base. Thanks to Duncan Murdoch -->

<!-- - nlsalt: attempt to mirror `nls() behaviour entirely in R. This is UNFINISHED.  -->
<!--   The effort showed that the structure of the programs was difficult to follow,  -->
<!--   undocumented, and unsuited to adding improvements like the Marquardt stabilization.  -->
<!--   We were able to get a pure-R version of `numericDeriv()` and rework most of the functions  -->
<!--   of `nlsModel` (but not `nlsModel.plinear`). This work may continue after the project -->
<!--   formally ends, but collaboration is likely needed with workers who have a deep knowledge -->
<!--   of R internals. `nls-flowchart.txt` was a start at documentation of the structure of `nls()`. -->

<!-- <!-- - nls-changes-for-small-residuals-in-nls-R-4.0.2.zip: collected material for the  --> 
<!-- <!--   fix by JN to the relative offset convergence criterion failure when there are small  --> 
<!-- <!--   residuals in problems sent to ``nls()`. Now part of R. --> 

<!-- - the safeguarded relative offset convergence criterion that allows small-residual problems -->
<!--   to ba handled with `nls()` is now in the R base code. -->

<!-- - `nlsj` is a refactoring of some `nls()` functionality. ?? What are key features?? -->
<!--   This is an interim package for exploration of ideas and will NOT, as it currently -->
<!--   is presented, become a distributed package. -->

<!-- - nlsralt: this is intended to be a modified version of Nash and Murdoch package  -->
<!--   `nlsr` to examine possible improvements discovered as a result of this project.  -->
<!--   This is EXPERIMENTAL. -->

<!-- For example,  -->
<!-- `nlsr` does not currently use the `subset` argument correctly. We also need to document -->
<!-- changes e.g., to numerical derivatives. However, these are tasks for JN rather than AB. -->


# Strategic choices in nonlinear model estimation

It is possible that `nls()` will remain more or less as it has been for the past
two decades. Given its importance to R, a focus of discussion should be what needs 
fixing and how that should be approached, and we welcome an opportunity to participate in 
such conversations. 

From a design point of view, a key difference in approach between `nls()` and 
`nlsr::nlxb()` is that `nls()` 
builds a large infrastructure, especially the `nlsModel()` function,  from which the Gauss-Newton
iteration can be executed and other statistical information such as profiles can be
computed, while `nlxb()` returns quite limited information, and 
computes what is needed on an "as and when" basis. This follows a path that one of us
(JN) established almost 50 years ago with the software that became @cnm79, separating
setup, solver, and post-solution analysis phases of computations. 
<!-- Here, that last  -->
<!-- phase is not part of `nlxb()`, but is the domain of other functions in the `nlsr` -->
<!-- package. -->


<!-- The `nls()` approach, as implemented in the base R code and `nlspkg` leads to considerable -->
<!-- entangling of the different functions and capabilities. Even after -->
<!-- both the preparation for and carrying out of the present project, we do not feel confident -->
<!-- to explain the code completely, nor to maintain it. However, we have made some  -->
<!-- forays into render parts of the code in R, and to understand where difficulties -->
<!-- lie. -->

## Opinions

To advance the stability and maintainability of R, we believe the program objects (R functions)
that are created by tools such as `nls()` should have minimal interactions and side-effects.
The aspects of `nls()` that have given us the most trouble:

- The functions that compute the residuals and jacobians often presume
  the data and parameters needed are available in a particular environment. As long
  as the correct environment is used, this provides a short syntax to invoke the
  calculations. The danger is that the wrong data is accessed if the internal search finds a
  valid name that is not the object we want.
  
- Weights, subsets, and various contextual controls such as that for the `na.action` option
  are similarly taken from the first available source. 
  Again, this makes for a very simple
  invocation of calculations, but the context is hidden from the user. Furthermore, it can be
  difficult for those of us trying to maintain or improve the code to be certain we have the
  context correct.
  
- The mixing of R, C and Fortran code was necessary for computational performance in the past.
  Lacking decent developer documentation, programmers now face a lot of work to fix or improve code. 
  We believe in having at least a working reference version of code in a single programming language. 
  If necessary, by
  measuring ("profiling") the code, we can find bottlenecks and, if necessary, substitute just those slower 
  parts of the reference code.

- A design that isolates the setup, solution, and post-solution parts of
  for complicated calculations reduces the number of objects that must be kept in 
  alignment.
  
- All iterative codes should return the best solution they have found so far, even if there
  are untoward conditions reached, such as a singular Jacobian. This modification could be
  made to `nls()` in the short term. 

- Given the existence of examples of good practices such as analytic derivatives, stabilized 
  solution of Gauss-Newton equations and bounds constrained parameters, base R tools should 
  be moving to incorporate them. 


<!-- ??fix next bits -->
<!-- - The different dialects of modeling expressions as detailed above should be rationalized. This -->
<!--   would, however, be a painful and mainly political activity. -->

<!-- - The tools for analytic derivatives that exist in parts of `nls()` for self-starting models and -->
<!--   in `nlsr` for problems specified as models that use expressions for which derivatives exist  -->
<!--   should be more generally available, but the task is one of tedium and limited reward to the -->
<!--   persons who do it, while benefiting the R community substantially. -->

<!-- - Similarly, default methods need stabilized solvers e.g., Marquardt methods, though to date -->
<!--   a mechanism to incorporate such modifications in `nls()` has eluded us. -->

<!-- - Bounds constraints are sufficiently important that they should be part of the default -->
<!--   methods -->
<!--   (rather than algorithm = "port" for `nls()`). `minpack.lm` should handle them correctly. -->



# Acknowledgement

Hans Werner Borchers has been helpful in developing the proposal for this project
and in comments on this and related work. Heather Turner co-mentored the project 
and helped guide the progress of the work.

# References
