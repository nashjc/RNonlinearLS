---
title: "Refactoring `nls()`: a constructive critique"
date: "2023-02-03"
abstract: >
   Based on work for a Google Summer of Code project "Improvements to `nls()`",
   we consider the features and limitations of the R function `nls()`
   in the context of improving and rationalizing R tools for nonlinear regression.
   Important considerations are the usability and  maintainability of
   the code base that provides the functionality `nls()` claims to offer.
   Our work suggests that the
   existing code makes maintenance and improvement very difficult,
   with legacy applications and examples blocking some important updates.
   Discussion of these matters is relevant to improving R generally as
   well as its nonlinear estimation tools.

draft: true
author:  
  # see ?rjournal_article for more information
    - name: John C. Nash
      affiliation: retired professor, University of Ottawa
      address:
      - Telfer School of Management
      - Ottawa ON Canada K1N 6N5
      orcid: 0000-0002-2762-8039
      email: profjcnash@gmail.com
    - name: Arkajyoti Bhattacharjee
      affiliation: Indian Institute of Technology
      address:
      - Department of Mathematics and Statistics
      - Kanpur
      email: arkastat98@gmail.com
type: package
output: 
  rjtools::rjournal_web_article:
    self_contained: yes
    toc: no
bibliography: ../BibSupport/ImproveNLS.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# The `nls()` function: strengths and shortcomings

`nls()` is the tool in base R, the primary software software distribution
from the Comprehensive R Archive Network (*https://cran.r-project.org*),
for estimating nonlinear statistical models. The function dates to the 1980s and the work
related to @bateswatts in S (see *https://en.wikipedia.org/wiki/S_%28programming_language%29*). 

The `nls()` function has a remarkable and comprehensive set of capabilities for estimating
nonlinear models that are expressed as **formulas**. In particular, we note that it

- handles formulas that include R functions, even ones which call calculations in
  other programming languages
- allows data to be weighted or subset
- can estimate bound constrained parameters
- provides a mechanism for handling partially linear models
- permits parameters to be indexed over a set of related data
- produces measures of variability (i.e., standard error estimates) for the estimated parameters
- has related profiling capabilities for exploring the likelihood surface as parameters are changed
- links to a number of pre-coded (`selfStart`) models 

With such a range of features and long history, the code has become untidy 
and overly patched. It is, to
our mind, essentially unmaintainable. Moreover, its underlying methods can and should be improved, 
as we suggest below. 

## Feature: Convergence and termination tests (FIXED)

A previous issue with `nls()` that prevented it from providing parameter estimates for zero-residual
(i.e., perfect fit) data was corrected thanks to suggestions by one of us. 

In the manual page for `nls()` in R 4.0.0 there is the warning:

>  **Do not use `nls` on artificial "zero-residual" data.**

>  The `nls` function uses a relative-offset convergence criterion
  that compares the numerical imprecision at the current parameter
  estimates to the residual sum-of-squares.  This performs well on data of
  the form 
  
>  $$  y = f(x, \theta) + eps $$
  
>  (with `var(eps) > 0`).  It fails to indicate convergence on data of the form

>  $$  y = f(x, \theta)  $$ 
  
>  because the criterion amounts to comparing two components of the round-off 
  error.

>  If you wish to test `nls` on artificial data please add a noise component, 
  as shown in the example below.

This amounted to admitting R cannot solve well-posed problems unless data is polluted
with errors. 

This issue can be easily resolved. The "termination test" for the **program** rather than for 
"convergence" of the underlying **algorithm** is the Relative Offset Convergence Criterion 
(see @BatesWatts81). This projects the proposed step in the parameter vector on the gradient 
and estimates how much the sum of squares loss function should decrease. This estimate is divided 
by the current size of the loss function to avoid scale issues.
When we have "converged", the estimated
decrease is very small. However, with small residuals,
the sum of squares loss function is (almost) zero and we get the possibility of a 
zero-divide failure. 

Adding a small quantity to the loss function before dividing avoids trouble. 
In 2021, one of us (J. Nash) proposed that `nls.control()` have an additional parameter `scaleOffset` 
with a default value of zero. Setting it to a small number -- 1.0 is 
a reasonable choice --
allows small-residual problems (i.e., near-exact fits) to be dealt with easily. We call this the
**safeguarded relative offset convergence criterion**. The default value gives the legacy behaviour.
We are pleased to note that this improvement is now in the R distributed code since version 4.1.0.


### More general termination tests

The convergence criterion above, the principal termination control of `nls()`, leaves out 
some possibilities that could
be useful for some problems. The package `nlsr` (@nlsr2019manual) already offers both 
the safeguarded relative offset test (**roffset**) as well as a **small sum of 
squares** test (**smallsstest**) that compares the latest evaluated sum of squared
(weighted) residuals to `e4` times the initial sum of squares, where
`e4 <- (100*.Machine$double.eps)^4` is approximately 2.43e-55.

We note that `nls()` uses a termination test to stop after `maxiter` "iterations". 
Unfortunately, the meaning of "iteration" varies among programs and requires 
careful examination of the
code. We prefer to use the number of times the residuals or the jacobian have
been computed and put upper limits on these. Our codes exit (terminate) when these limits
are reached. Generally we prefer larger limits than the default `maxiter = 50` of `nls()`,
but that may reflect the more difficult problems we have encountered because 
users consult us when standard tools have given unsatisfactory results. 


## Feature: Failure when Jacobian is computationally singular

This refers to the infamous "singular gradient" termination message of `nls()`. 
A Google search of 

```
R nls "singular gradient"
```

gets over 4000 hits that are spread over some years. This could be because the 
Jacobian is poorly approximated (see **Jacobian computation** below). However,
it is common in nonlinear least squares computations that the Jacobian is very close
to singular for some values of the model parameters. In such cases, we need to find
an alternative algorithm to the Gauss-Newton iteration of `nls()`. The most common
work-around is the Levenberg-Marquardt stabilization (see @Marquardt1963, @Levenberg1944, 
@jn77ima), and versions of it have been implemented in packages `minpack.lm` and `nlsr`.
In our work, we prepared experimental prototypes of `nls` that incorporate
stabilization, but integration with all the features of `nls()` has proved difficult.

Let us consider a 'singular gradient' example 
*https://stats.stackexchange.com/questions/13053/singular-gradient-error-in-nls-with-correct-starting-values*.
`nlsr::nlxb()` displays the singular values of the Jacobian, which confirm that the `nls()` error message is correct,
but this is not helpful in obtaining a solution.

```{r SESingGrad}
reala <- -3; realb <- 5; realc <- 0.5; realr <- 0.7; realm <- 1 # underlying parameters
x <- 1:11 # x values; 11 data points
y <- reala + realb * realr^(x - realm) + realc * x # linear + exponential function
testdat <- data.frame(x, y) # save in a data frame
strt <- list(a = -3, b = 5, c = 0.5, r = 0.7, m = 1) # give programs a good start
jform <- y ~ a + b * r^(x - m) + c * x # Formula
library(nlsr)
linexp2 <- try(nlxb(jform, data = testdat, start = strt, trace = FALSE))
linexp2 # Note singular values of Jacobian in rightmost column
# We get an error with nls()
linexp <- try(nls(jform, data = testdat, start = strt, trace = FALSE))
```

The Jacobian is essentially singular as shown by its singular
values. Note that these are **displayed** by package `nlsr` in a single column in
the output to provide a compact layout, but the values do **NOT** correspond to 
the individual parameters in 
whose row they appear; they are a property of the whole problem.

## Feature: Jacobian computation

`nls()`, with the `numericDeriv()` function, computes the Jacobian as the "gradient"
attribute of the residual vector. This is implemented as a mix of R and C code, but
we have created a rather more compact version entirely in R. 
See *https://github.com/nashjc/RNonlinearLS/tree/main/DerivsNLS*.
<!-- Code project. See the companion document **Jacobian Calculations for nls()** -->
<!-- (?? need citation -- currently **DerivsNLS.pdf**). -->

As far as we can understand the logic in `nls()`, the Jacobian computation during
parameter estimation is carried out entirely within the called C-language program,
and the R code function `numericDeriv()`, part of `./src/library/stats/R/nls.R`
in the R distribution source code. This is used to provide Jacobian information in 
the `nlsModel()` and `nlsModel.plinear()` functions, which are **not** exported for 
general use.

The Jacobian used by `nlsr::nlxb()` is, by default, computed from analytic expressions, 
while that
of `nls()` mostly uses a numerical approximation. Some `selfStart` models do provide
"gradient" information. `minpack.lm::nlsLM()` invokes `numericDeriv()` in its local 
version of `nlsModel()`, but it appears to use an internal approximate Jacobian code from 
the original Fortran `minpack` code, namely, `lmdif.f`. Such differences in approach
can lead to different behaviour, though in our experience differences are usually minor.

- A pasture regrowth problem (@Huet2004, page 1, based on @Ratkowsky1983) has a poorly
  conditioned Jacobian and `nls()` fails with "singular gradient". 
  Worse, numerical approximation to the Jacobian give the error
  "singular gradient matrix at initial parameter estimates" for `minpack.lm::nlsLM`
  so that the Marquardt stabilization is unable to take effect, while the analytic
  derivatives of `nlsr::nlxb` give a solution. 

- Karl Schilling (private communication) provided an example where a model specified
  with the formula `y ~ a * (x Ë† b)` causes `nlsr::nlxb` to fail because the partial
  derivative w.r.t. `b` is `a * (x^b * log(x))`. If there is data for which `x = 0`,
  this is undefined. In such cases, we observed that `nls()` and `minpack.lm::nlsLM`
  found a solution, though it can be debated whether such lucky accidents can be
  taken as an advantage.

Note that the `selfStart` models in the base R `./src/library/stats/R/zzModels.R` file 
and in package `nlraa` (@MiguezNLRAA2021) may
provide the Jacobian in the "gradient" attribute of the "one-sided" formula that defines
each model, and these Jacobians may be the analytic forms. The `nls()` function, after
computing the "right hand side" or `rhs` of the residual, then checks to see if the
"gradient" attribute is defined, and, if not, uses `numericDeriv` to compute a Jacobian
into that attribute. This code is within the `nlsModel()` or `nlsModel.plinear()`
functions. The use of analytic Jacobians
almost certainly contributes to the good performance of `nls()` on `selfStart` models.

## Feature: Weights on observations

`nls()`, `nlsr::nlxb()` and `minpack.lm::nlsLM()` all have a `weights` argument that
specifies a vector of weights the same length as the number of residuals. Each residual
is multiplied by the square root of the corresponding weight. The values returned by the
`residuals()` function are weighted, and the `fitted()` or `predict()` function are 
used to compute raw residuals.


## Feature: Subsetting

`nls()` accepts an argument `subset`. This acts through the mediation of
`model.frame` and is not clearly obvious in the source code files `/src/library/stats/R/nls.R` and 
`/src/library/stats/src/nls.C`. 

While the implementation of subset at the level of the call to `nls()` has a certain
attractiveness, it does mean that the programmer of the solver needs to be aware of the
source (and value) of objects such as the data, residuals and Jacobian. This is overly
complicated. By preference, 
we would implement subsetting by means of zero-value weights, with observation counts
(and degrees of freedom) computed via the numbers of non-zero weights. Alternatively,
we would extract a working dataframe from the relevant elements in the original.


## Feature: na.action

`na.action` is an argument to the `nls()` function, but it does not appear obviously in the
source code, often being handled behind the scenes after referencing the option `na.action`.
A useful, but possibly dated, description is given in:
*https://stats.idre.ucla.edu/r/faq/how-does-r-handle-missing-values/*.

The typical default action, which can be seen by using the command `getOption("na.action")`
is `na.omit`. This option essentially omits from computations any observations
containing missing values (i.e. any row of a data frame containing an NA). 
`na.exclude` does much of the same for computations, but keeps the rows with NA elements so 
that predictions are in the correct row position. We recommend that workers actually test 
output to verify the behaviour is as wanted.

A succinct description of the issue is given in: *https://stats.stackexchange.com/questions/492955/should-i-use-na-omit-or-na-exclude-in-a-linear-model-in-r*
where the "Answer" states

> The only benefit of `na.exclude` over `na.omit` is that the former will retain the 
original number of rows in the data. This may be useful where you need to retain the 
original size of the dataset - for example it is useful when you want to compare 
predicted values to original values. With `na.omit` you will end up with fewer 
rows so you won't as easily be able to compare.

`na.pass` simply passes on data "as is", while `na.fail` will essentially stop if any missing
values are present.

Our concern with `na.action` is that users may be unaware of the effects of an option
setting they may not even be aware has been set. Should `na.fail` be the default?


## Feature: model frame

`model` is an argument to the `nls()` function, which is documented as:

> **model** logical. If true, the model frame is returned as part of the object. Default is FALSE.

Indeed, the argument only gets used when `nls()` is about to return its result object, and the
element `model` is NULL unless the calling argument `model` is TRUE. (Using the same name could
be confusing.) However, the model frame is used within the function code in the form of the object
`mf`. We feel that users could benefit from more extensive documentation and examples of its use.

## Feature: Sources of data

`nls()` can be called without specifying the `data` argument. In this case, it will
search in the available environments (i.e., workspaces) for suitable data objects. 
We do NOT like this approach, but it is "the R way". R allows users to leave many 
objects in the default (`.GlobalEnv`) workspace. Moreover, users have to actively 
suppress saving this workspace (`.RData`) on exit; otherwise, any such file in the path, 
when R is launched, will be loaded. The overwhelming proportion of R users in our
acquaintance avoid saving the workspace because of the danger of lurking data and
functions which may cause unwanted results.

Nevertheless, to provide compatible behaviour with `nls()`, competing programs 
need to ensure equivalent behaviour, but users should test that the 
operation is as they intend. 

## Feature: missing start vector and self-starting models

Nonlinear estimation algorithms are almost all iterative and need a set of starting
parameters. `nls()` offers a special class of modeling functions called **`selfStart`** 
models. There are a number of these in base R (`./src/library/stats/R/zzModels.R`)
and others in R packages such as CRAN package `nlraa` (@MiguezNLRAA2021), as well as 
the now-archived package `NRAIA`. Unfortunately, some **`selfStart`** codes
entangle the calculation of starting values for parameters
with the particulars of the `nls()` code. Though there is a `getInitial()` function,
this is not easy to use to simply compute the initial parameter estimates outside of
`nls()`, in part 
because it may call that function. Such circular references are, in our view, dangerous.
Moreover, we believe that it would be helpful to have `selfStart` models that allow
users to explicitly provide values other than those suggested for the starting parameters.

Other concerns are revealed by the example below. Here, the `SSlogis` `selfStart` function 
is used to generate a set
of initial parameters for a 3-parameter logistic curve. The form used by `SSlogis`
is $y \,\sim\, Asym\,/\, (1 \,+\, exp((xmid\,-\,tt)\,/\,scal))$, but we show how the starting parameters
for this model can be transformed to those of 
another form of the model, namely, $y \,\sim\, b1\,/\,(1 \,+\, b2\,*\,exp(-b3\,*\,t))$.

The code for `SSlogis()` is in `./src/library/stats/R/zzModels.R`.
<!-- ``` -->
<!-- SSlogis <- selfStart(~ Asym/(1 + exp((xmid - input)/scal)), -->
<!--     selfStart( -->
<!--         function(input, Asym, xmid, scal) -->
<!--         { -->
<!--               .expr1 <- xmid - input -->
<!--               .expr3 <- exp(.e2 <- .expr1/scal) -->
<!--               .expr4 <- 1 + .expr3 -->
<!--               .value <- Asym/.expr4 -->
<!--               .actualArgs <- as.list(match.call()[c("Asym", "xmid", "scal")]) -->
<!--               if(all(vapply(.actualArgs, is.name, NA))) -->
<!--               { -->
<!--             		  .expr10 <- .expr4^2 -->
<!--                   .grad <- array(0, c(length(.value), 3L), list(NULL, c("Asym", "xmid", "scal"))) -->
<!--                   .grad[, "Asym"] <- 1/.expr4 -->
<!-- 		              .grad[, "xmid"] <- - (xm <- Asym * .expr3/scal/.expr10) -->
<!-- 		              .grad[, "scal"] <- xm * .e2 -->
<!--                   dimnames(.grad) <- list(NULL, .actualArgs) -->
<!--                   attr(.value, "gradient") <- .grad -->
<!--               } -->
<!--               .value -->
<!--         }, -->
<!--         initial = function(mCall, data, LHS, ...) { -->
<!--               xy <- sortedXyData(mCall[["input"]], LHS, data) -->
<!--               if(nrow(xy) < 4) { -->
<!--                   stop("too few distinct input values to fit a logistic model") -->
<!--               } -->
<!--               z <- xy[["y"]] -->
<!--               ## transform to proportion, i.e. in (0,1) : -->
<!--               rng <- range(z); dz <- diff(rng) -->
<!--               z <- (z - rng[1L] + 0.05 * dz)/(1.1 * dz) -->
<!--               xy[["z"]] <- log(z/(1 - z))		# logit transformation -->
<!--               aux <- coef(lm(x ~ z, xy)) -->
<!--               pars <- coef(nls(y ~ 1/(1 + exp((xmid - x)/scal)), -->
<!--                                data = xy, -->
<!--                                start = list(xmid = aux[[1L]], scal = aux[[2L]]), -->
<!--                                algorithm = "plinear", ...)) -->
<!--               setNames(pars [c(".lin", "xmid", "scal")], -->
<!--                        mCall[c("Asym", "xmid", "scal")]) -->
<!--         }, -->
<!--         parameters = c("Asym", "xmid", "scal")) -->
<!-- ``` -->
This R function includes analytic expressions for the Jacobian ("gradient").
These could be useful to R users, especially if documented. Moreover, we
wonder why the programmers have chosen to save so many quantities in "hidden"
variables, i.e., with names preceded by ".". These are then not displayed by the `ls()`
command, making them difficult to access.

In the event that a `selfStart` model is not available, `nls()` sets all the starting parameters 
to 1. This is, in our view, tolerable, but could be improved by using a set of values
that are all slightly different, which, in the case of the example model $y \,\sim\, a \,*\, exp(-b \,*\, x) + c\,*\,exp(-d \,*\, x)$
would avoid a singular Jacobian because $b$ and $d$ were equal in value. 
A sequence like  1.0, 1.1, 1.2, 1.3 for the four parameters 
could be provided quite simply instead of all 1's.

<!-- ```{r hobbsSSlogis} -->
<!-- weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, -->
<!--           38.558, 50.156, 62.948, 75.995, 91.972) -->
<!-- tt <- 1:12 -->

<!-- NLSformula0 <- y ~ b1/(1+b2*exp(-b3*tt)) -->
<!-- NLSformula <- y ~ SSlogis(tt, Asym, xmid, scal) -->
<!-- NLSformulax <- y ~ Asym/(1+exp((xmid-tt)/scal)) -->
<!-- NLStestdata <- data.frame(y=weed, tt=tt)  -->
<!-- s0 <- getInitial(NLSformula, NLStestdata) -->
<!-- print(s0) -->
<!-- # We transform the parameters for the NLSformula0 model of original specification. -->
<!-- s1<-list(b1=s0[1], b2=exp(s0[2]/s0[3]), b3=1/s0[3]) -->
<!-- print(as.numeric(s1)) -->
<!-- # No actual improvement because nls() has been already used to get the starting values, -->
<!-- # but we do get SEs -->
<!-- hobblog<-nls(NLSformula0, data=NLStestdata, start=s1) -->
<!-- summary(hobblog) -->
<!-- deviance(hobblog) -->
<!-- # nls fails without selfStart -- singular gradient, even on stabilized formula -->
<!-- try(hobblogx<-nls(NLSformulax, data=NLStestdata)) -->
<!-- # But Marquardt is able to get a solution easily -->
<!-- library(nlsr) -->
<!-- hobblogxx<-nlxb(NLSformulax, data=NLStestdata, start=c(Asym=1, xmid=1, scal=1)) -->
<!-- hobblogxx -->
<!-- ``` -->

<!-- ### selfStart models in base R -->

<!-- The following models are provided (in file ./src/library/stats/R/zzModels.R) -->

<!-- ``` -->
<!-- SSasymp         - asymptotic regression model -->
<!-- SSasympOff      - alternate formulation of asymptotic regression model with offset -->
<!-- SSasympOrig     - exponential curve through the origin to an asymptote -->
<!-- SSbiexp         - y ~ ~ A1 * exp(-exp(lrc1)*input) + A2 * exp(-exp(lrc2) * input) -->
<!-- SSfol           - y ~ Dose * (exp(lKe + lKa - lCl) * (exp(-exp(lKe) * input) - -->
<!--                           exp(-exp(lKa) * input))/(exp(lKa) - exp(lKe))) -->
<!-- SSfpl           - four parameter logistic model -->
<!-- SSlogis         - three parameter logistic model -->
<!-- SSmicmen        - Michaelis-Menten model for enzyme kinetics -->
<!-- SSgompertz2     - Gompertz model for growth curve data -->
<!-- SSweibull       - Weibull model for growth curve data -->
<!-- ``` -->


### Strategic issues in `selfStart` models

By providing starting values that are likely to be reasonable and by including 
Jacobian code in the `selfStart` model code, some of the deficiencies of the 
Gauss-Newton algorithm are mitigated. However, creating such functions is a lot 
of work, and their
documentation (file `./src/library/stats/man/selfStart.Rd`) is quite complicated.
We believe that the focus should be placed on getting good initial parameters,
that is `getInitial()` function, though avoiding the current calls back to `nls()`.
Interactive tools, such as "visual fitting" (@nash1996nonlinear) might be
worth considering.

We also note that the introduction of `scaleOffset` in R 4.1.1 to deal with the 
convergence test for small residual problems now requires that the `getInitial()`
function have dot-arguments (`...`) in its argument list. This illustrates the
entanglement of many features in `nls()`.

## Issue: returned output of nls() and its documentation

The output of `nls()` is an object of class "nls" which has the following structure:

>A list of

>`m` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  an `nlsModel` object incorporating the model.

>`data`	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;the expression that was passed to `nls` as the data argument. The actual \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$  data values are present in the environment of the `m` components, e.g., \newline $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ `environment(m$conv)`.

>`call` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the matched call with several components, notably `algorithm`.

>`na.action` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the `"na.action"` attribute (if any) of the model frame.

>`dataClasses` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $~$the `"dataClasses"` attribute (if any) of the "`terms`" attribute of the \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ model frame.

>`model` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;if `model = TRUE`, the model frame.

>`weights` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if `weights` is supplied, the weights.

>`convInfo` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; a list with convergence information.

>`control` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the control `list` used, see the `control` argument.

>`convergence, message` &nbsp;for an `algorithm = "port"` fit only, a convergence code (`0` for  \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$convergence) and message.

> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; To use these is *deprecated*, as they are available from `convInfo` now.

                  -----------------------------------------------------------

<!-- ### Example output -->

<!-- To illustrate, let us run the Croucher example. -->

<!-- ```{r nlsoutx} -->
<!-- # Croucher example -->
<!-- xdata <- c(-2,-1.64,-1.33,-0.7,0,0.45,1.2,1.64,2.32,2.9) -->
<!-- ydata <- c(0.699369,0.700462,0.695354,1.03905,1.97389,2.41143,1.91091,0.919576,-0.730975,-1.42001) -->
<!-- p1<- 1; p2<-0.2; NLSstart<-list(p1=p1,p2=p2) -->
<!-- NLSformula <- ydata ~ p1*cos(p2*xdata) + p2*sin(p1*xdata) -->
<!-- NLSdata<-data.frame(xdata, ydata) -->
<!-- # Try full output version of nls -->
<!-- library(nlspkg) # use the packaged version of nls() -->
<!-- result<-nls(NLSformula, data=NLSdata, start=NLSstart, model=TRUE) -->
<!-- # We can display the result with several commands -->
<!-- # str(result) -- displays large amount of material - suppressed here -->
<!-- #    as it is too wide for the page -->
<!-- # result # displays the object -->
<!-- # ls(result) # to list the elements of the output -->
<!-- # ls(result$m) # and in particular the "m" object -->
<!-- ``` -->

<!-- ### Concerns with content of the nls result object -->

The `nls` object contains some elements that are awkward to produce by other algorithms,
but some information that would be useful is not presented in a clear manner. Moreover, the
complexity of the object is a challenge to users.

In the following, we use `result` as the returned object from `nls()`.

The `data` return element is an R symbol. To actually access the data from this 
element, we need to use the syntax:

```
eval(parse(text=result$data))
```

However, if the call is made with `model=TRUE`, then there is a returned element
`model` which contains the data, and we can list its contents using:

```
ls(result$model)
```

and if there is an element called `xdata`, it can be accessed as `result$model$xdata`.

Let us compare the `nls()` result with that from 
`nlsr::nlxb()`, which ostensibly solves the same problem:


>`coefficients`    A named vector giving the parameter values at the supposed solution.

>`ssquares` $~~~~~$ The sum of squared residuals at this set of parameters.

>`resid` $~~~~~~~~~~$ The residual vector at the returned parameters.

>`jacobian` $~~~~~$ The jacobian matrix (partial derivatives of residuals w.r.t. the 
                parameters) at the \newline
$~~~~~~~~~~~~~~~~~~~~$ returned parameters.

>`feval` $~~~~~~~~~~$ The number of residual evaluations (sum of squares computations) used.

>`jeval` $~~~~~~~~~~$ The number of Jacobian evaluations used.


However, actually looking at the structure of a returned result gives a list of 11
items. The extra 5 are:

```
 $ lower       : num [1:3] -Inf -Inf -Inf
 $ upper       : num [1:3] Inf Inf Inf
 $ maskidx     : int(0) 
 $ weights     : NULL
 $ formula     :Class 'formula'  language y ~ Asym/(1 + exp((xmid - tt)/scal))
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
 - attr(*, "class")= chr "nlsr"
```
The result object from `nlsr::nlxb()` is still much smaller than the one `nls()` returns. 
Moreover, `nlxb`
explicitly returns the sum of squares as well as the residual vector and Jacobian. 
The counts of evaluations are also returned.
(Note that the singular values of the Jacobian are
actually computed in the `print` and `summary` methods for the result.) 
As we made our comparisons, we noted several potential updates to the `nlsr` documentation
as well as that for `nls()`.

### Weights in returned functions from nls()

As already noted, the function `resid()` (an alias for `residuals()`) is WEIGHTED, as are those in the
structured object `m` returned by `nls()` or `minpack.lm::nlsLM`,
e.g., `ans$m$resid()`. The function `nlsModel()`, which we have had to extract from the 
base R code and explicitly `source()` because it is not exported to the working namespace,
allows us to compute residuals for particular coefficient sets.

```{r nlswtx, echo=TRUE}
weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972)
tt <- 1:12
weeddf <- data.frame(tt, weed)
wts <- 0.5^tt # simple weights
frmlogis <- weed ~ Asym/(1 + exp((xmid - tt)/scal))
Asym<-1; xmid<-1; scal<-1
nowt<-nls(weed ~ SSlogis(tt, Asym, xmid, scal)) # UNWEIGHTED
nowt
nowt$m$resid() # This has UNWEIGHTED residual and Jacobian. Does NOT take coefficients.
usewt <- nls(weed ~ SSlogis(tt, Asym, xmid, scal), weights=wts)
usewt
usewt$m$resid() # WEIGHTED. Does NOT take coefficients.
source("nlsModel.R")
nmod0 <- nlsModel(frmlogis, data=weeddf, start=c(Asym=1, xmid=1, scal=1), wts=wts)
nmod0$resid() # Parameters are supplied in nlsModel() `start` above.
nmod <- nlsModel(frmlogis, data=weeddf, start=coef(usewt), wts=wts)
nmod$resid()
```




# References

?? to sort out
- note that AB now at Ohio State?
- ORCID for AB?
