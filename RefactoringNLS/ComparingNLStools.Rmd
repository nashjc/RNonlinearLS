---
title: "A comparison on R tools for nonlinear least squares modeling"
author: 
   - John C. Nash \thanks{ retired professor, Telfer School of Management, University of Ottawa}
   - Arkajyoti Bhattacharjee \thanks{Department of Mathematics and Statistics, Indian Institute of Technology, Kanpur}
date: "2023-3-2"
output: 
    pdf_document:
        keep_tex: true
        toc: true
bibliography: ./ImproveNLS.bib
link-citations: yes
linkcolor: red
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## require(bookdown) # language engine to display text - does not seem necessary
```

<!-- ?? nlme::gnls() -- should we note it? -->

# Abstract

In a Google Summer of Code project "Improvements to `nls()`",
we considered the features and limitations of the R function `nls()`
in the context of improving and rationalizing R tools for nonlinear regression.
The rich features of this function are weakened by a number of deficiencies and
inconsistencies, in particular a lack of stabilization of the Gauss-Newton solver.
Further considerations are the usability and  maintainability of
the code base that provides the functionality `nls()` claims to offer.
Various packages, including our `nlsr`, provide alternative capabilities.
This article considers the differences in goals, approach and features of
different tools for nonlinear least squares modeling in R. Discussion of 
these matters is relevant to improving R generally as well as its 
nonlinear estimation tools.


## The `nls()` function

`nls()` is the tool in base R, the primary software software distribution
from the Comprehensive R Archive Network (*https://cran.r-project.org*),
for estimating nonlinear statistical models. The function dates to the 1980s and the 
work related to @bateswatts in S (see 
*https://en.wikipedia.org/wiki/S_%28programming_language%29*). 

The `nls()` function has a remarkable and comprehensive set of capabilities for 
estimating nonlinear models that are expressed as **formulas**. In particular, we
note that it

- handles formulas that include R functions, even ones which call calculations in
  other programming languages
- allows data to be weighted or subset
- can estimate bound constrained parameters
- provides a mechanism for handling partially linear models
- permits parameters to be indexed over a set of related data
- produces measures of variability (i.e., standard error estimates) for the 
estimated parameters
- has related profiling capabilities for exploring the likelihood surface as 
parameters are changed
- links to a number of pre-coded (`selfStart`) models that do not require
  initial parameter values.

With such a range of features and long history, the code has become untidy 
and overly patched, difficult to maintain, and its underlying methods could
be improved. Various workers have developed packages to overcome these 
concerns, and we will address some of these here.

## Scope of our comparison

The tools we will compare are the base-R `nls()` function 
and a number of packages that are available on the CRAN repository.
Of these, we will pay particular attention to **nlsr** (@nlsr2023manual),
**minpack.lm** (@minpacklm12), 
gslnls (@gslnls23), and nls2 (@nls2-22).

Other CRAN packages worth Commentary: ??

??put in proper refs to each

onls -- when there are errors in variables, this minimizes the orthogonal 
residuals

crsnls -- Controlled random search. No vignette!

NISTnls -- NIST problems for testing nls methods

nlshelper -- A few utilities for summarizing, testing, and plotting non-linear
    regression models fit with nls(), nlsList() or nlme()

nlsic -- We solve non linear least squares problems with optional
    equality and/or inequality constraints.
    
nlsMicrobio -- Data sets and nonlinear regression models dedicated to predictive microbiology.

nlsmsn -- Fit univariate non-linear scale mixture of skew-normal(NL-SMSN) regression, details in Garay, Lachos and Abanto-Valle (2011)

nls.multstart -- Non-linear least squares regression with the Levenberg-Marquardt algorithm using multiple starting values for increasing the chance that the minimum found is the global minimum.

nlstac -- Set of functions implementing the algorithm described in Fernandez 
    Torvisco et al. (2018) for fitting separable nonlinear regression curves.
    ?? check ref and add to bib.??

nlstools -- Several tools for assessing the quality of fit of a gaussian nonlinear model are provided.

easynls -- Fit and plot some nonlinear models. ?? no vignette!

nlraa -- a set of nonlinear *selfStart* models. Most include analytic Jacobian
code. (@MiguezNLRAA2021)

optimx -- not really a nonlinear least squares method, but has general optimizers
that can be applied to minimize a nonlinear function e.g., a nonlinear sum of 
squares.

It is likely that there are a number of other tools that could be relevant to
this comparison in the Bioconductor (@Bioconductor) collection 
and on repositories like
Github (https://github.com) and Gitlab (https://about.gitlab.com), but we will 
not pursue those possibilities here.

## Comparison dimensions

The tools for nonlinear estimation we consider have been built with different
goals and contexts. We will make our comparison in a number of directions.
We will make a number of specific notes, but our main commentary will consider
the following categories. These overlap, for example in that the emphasis on modeling
versus solving the nonlinear least squares problem will result in different
output.f

### Modeling versus minimizing a sum of squares

The goal of many statisticians and other workers is to estimate a model. The
minimization of a sum of squared residuals -- the nonlinear least squares problem --
is the mechanism to estimate the model. Some of the tools we review focus on the
modeling, so attempt to provide measures of uncertainty and other information. 
The interface to modeling tools will generally try to streamline that task. By
contrast, nonlinear least squares tools are trying to solve the more limited
numerical task.

### Programming language(s)

R tools are our interest, but they often call programs in other programming
languages. There are advantages in using a well-established program that has
been much used and tested. Early versions of R were relatively sluggish in 
computational speed. The downside is that we must interface between two or more
programming languages, adding to the risk of errors. Changes to one or the 
other language or its library support may
force maintenance issues. Moreover, the pool of expertise with languages like C
and Fortran is diminishing.

### Internal solver stabilization

Early nonlinear least squares programs generally used the Gauss-Newton method 
(@Hartley1961). This is often very effective and efficient, but for a quite large 
proportion of problems can fail to find the solution. Stabilization of the 
computations is relatively straightforward in principle, and overcomes most
of the failures, though sometimes at the cost of more calculations.

### Jacobian calculation

The necessarily iterative solution of a nonlinear least squares problem 
to carry out nonlinear modeling requires that we can find smaller values
of a sum of squared residuals. This implies there is a "downhill" direction.
The gradient of the residual sum of squares is computed as the product
of the Jacobian matrix: the partial derivatives of the residual vector 
with respect to the modeling parameters.
(?? do we need to present this, or give a ref)

Most researchers find the effort to interface the differential calculus 
required to specify the Jacobian to a nonlinear estimation tool very 
challenging. The work-around is generally to use a numerical approximation,
but there is the possibility of using symbolic or automatic differentiation,
and there are particular models where workers have provided code for the
Jacobian.

### Algorithm and termination choices

Iterative algorithms need to be stopped at some point, and how termination
is forced may alter results. Furthermore, even supposedly similar algorithms
may differ greatly in detail. 

### Constraints on parameters

Reality often imposes conditions on parameters. While general constraints can
be difficult to include, bounds on parameters can be accommodated by many 
solvers. For some reason, fixed parameters (masks) are not often considered,
but are very convenient when a parameter has a commonly accepted value that we
later wish to allow to be estimated from the data.

### Special capabilities


?? weighting, indexed parameters, partially linear ...

### Output 

While the aim of nonlinear estimation is to return a set of parameters for a
particular model, there is a variety of additional information which may be
provided. From the perspective of the nonlinear least squares problem, there
are measures of work and information on the quality of the solution, while
for statistical purposes measures of uncertainty in the parameters are helpful.

Some tools allow constraints to be imposed on the solution, and it is helpful
to know if these constraints are active at the proposed solution.

?? DO WE WANT A COMPARISON TABLE

### Documentation

?? presence of vignettes, demos, (good?) examples


## An illustrative example

The Hobbs weed infestation problem (@cnm79 [page 120]) is a growth curve modeling 
task which is seemingly straightforward but turns out to be quite nasty. Because
this problem has a very succinct statement, it often provides the "short 
reproducible example" much beseeched of contributors to R mailing lists. 
Nevertheless, this is not an artificially created test problem, but one that
came from a field researcher.

The data and a graph of it is given below.

```{r ex01, echo=TRUE}
weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972)
tt <- 1:12
weeddf <- data.frame(tt, weed)
plot(weeddf, main="Hobbs weed infestation data")
```

Three suggested models for this data are (with names to allow for easy reference)

**Logistic3U**:
$$  y \approx  b_1 / (1 + b_2 * exp(- b_3 * t)) $$

**Logistic3S**:
$$ y \approx  100 * c_1 / (1 + 10 * c_2 * exp(- 0.1 * c_3 * t)) $$

**Logistic3T**:
$$ y \approx Asym / (1 + exp((xmid - t)/scal)) $$

The functions above are equivalent, but the first is generally more awkward
to solve numerically due to its poor scaling. The parameters of the three forms
are related as follows:

$$   Asym =  b_1 = 100 * c_1 $$
$$ exp(xmid/scal)  =  b_2 = 10 * c_2 $$
$$ 1/scal  =  b_3 $$
To allow for simpler discussion, let us say that the parameters for a vector
$p$ and the model function is called $model(p)$. The residuals can be written
either as 

$$ r_1 = y - model(p)$$

or

$$ r = model(p) - y $$

since when squared the loss function has an identical value. In both cases, the
residual vector is a function of $p$. The second form
has a minor advantage in avoiding a potential sign error 
if we need to differentiate to get gradients etc. 

### The loss function and its minimization

The general approach of nonlinear optimizers, of which nonlinear least squares
solvers are usually a subset, is to try to reduce the loss function (or objective
function) defined by the sum of squares of the residuals ($y - model()$) by 
altering the model parameters from some starting guess. In all general 
methods of which we are aware, this is an iterative process. 

Let us consider there are $n$ parameters and $m$ residuals. 
We note several useful quantities

Loss function:
$$ S(p) = r' r = \sum_{i=1}^m { r_i^2 }$$
Gradient of $S(p)$:
$$ 2 * J' r$$
where the Jacobian $J$ is given by elements
$$ J_{i,j} = \partial r_i / \partial p_j $$
and the Hessian is defined by elements
$$ H_{i,j} = \partial ^2 S(p) / {\partial p_i \partial p_j} $$
If we expand the Hessian for nonlinear least squares problems, we find

$$  H_{i,j} = \sum_{k = 1}^m{ J_{k,i} J_{k,j}}  + \sum_{k = 1}^m {r_k * \partial{r_k}/{\partial p_i \partial p_j}}$$
Let us use $D_{i,j}$ for the elements of the second term of this expression.
What is generally called **Newton's method** for function minimization tries
to set the gradient to zero (to find a stationary point of the function $S(p)$).
This leads to the **Newton's equation**

$$ H \delta = -g $$
given a set of parameters $p$, we solve this equation for $\delta$, adjust $p$
to $p + \delta$ and iterate, hopefully to converge on a solution.

Applying this to a sum of squares problem gives

$$ H \delta = (J'J + D) \delta = - J' r$$
In this expression, only the elements of $D$ have second partial derivatives. 
Gauss, attempting to model planetary orbits, had small residuals, and noted
that these multiplied the second partial derivatives of $r$, so he approximated

$$ H \approx J' J$$
by assuming $D \approx 0$. This results in the Gauss-Newton method where we
solve 

$$ J' J \delta = - J' r$$
though we can avoid some loss of accuracy by NOT forming the inner product matrix
$J' J$ by solving the linear least squares matrix problem

$$ J \delta \approx -r $$
which may be accomplished by a variety of matrix decomposition methods. 

In reality, there are many problems were $D$ should not be ignored, but the work
to compute it precisely is considerable. A number of work-arounds have been 
proposed, of which the Levenberg-Marquardt (@Levenberg1944, @Marquardt1963) is
the most commonly used. Henceforth we will simply say "Marquardt" as we believe
he was the first to actually incorporate the ideas into a practical computer
program. 

The suggestion is that $D$ be replaced by a multiple of the unit matrix or else
a multiple of the diagonal part of $J' J$. For machines with exceptionally low
precision arithmetic found on some minicomputers in the 1970s, one of us suggested
a linear combination of these ideas, as with low precision, some elements of 
$J' J$ could underflow to zero (@jn77ima). We note that the various choices for
$D$, especially the scaling multiple, as well as possible line search along the
direction $\delta$ rather than a unit step (@Hartley1961), 
give rise to a panoply of variant
algorithms. "Marquardt's method" is a family of methods. Fortunately, most choices
work well.

### Problem setup

Let us specify in R the three model formulae and set some starting
values for parameters. These starting points are NOT equivalent, and
are deliberately crude choices. Workers performing many calculations 
of a similar nature should try to provide good starting points to reduce
computation time and the chance a false solution is found.
  
```{r ex02set, echo=TRUE}
# model formulas
frmu <- weed ~ b1/(1+b2*exp(-b3*tt))
frms <- weed ~ 100*c1/(1+10*c2*exp(-0.1*c3*tt))
frmt <- weed ~ Asym /(1 + exp((xmid-tt)/scal))
#
# Starting parameter sets
stu1<-c(b1=1, b2=1, b3=1)
sts1<-c(c1=1, c2=1, c3=1)
stt1<-c(Asym=1, xmid=1, scal=1)
```

One of the useful features of `nls()` is the possibility of a `selfStart` model,
where starting parameter values are not required. We discuss aspects of this
feature at several points below. However, in the event that a `selfStart` model 
is not available, `nls()` sets all the starting parameters 
to 1. This is, in our view, tolerable, but could be improved by using a set of values
that are all slightly different, which, in the case of the example 
model $y \,\sim\, a \,*\, exp(-b \,*\, x) + c\,*\,exp(-d \,*\, x)$
would avoid a singular Jacobian because $b$ and $d$ were equal in value. 
Program modifications to give a sequence like  1.0, 1.1, 1.2, 1.3 for the four 
parameters are fairly obvious. However, users may want to avoid equal parameter
starts in situations like this when setting starting values.

It is also possible to provide R functions for the residual and Jacobian. 
This is usually more work for the user if the formula setup is possible.
To illustrate, we show the functions for the unscaled 3 parameter logistic.

```{r ex02fn, echo=TRUE}
# Logistic3U
hobbs.res  <-  function(x){ # scaled Hobbs weeds problem -- residual
  # This variant uses looping
  if(length(x) != 3) stop("hobbs.res -- parameter vector n!=3")
  y  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 
           38.558, 50.156, 62.948, 75.995, 91.972)
  tt  <-  1:12
  res  <-  x[1]/(1+x[2]*exp(-x[3]*tt)) - y
}

hobbs.jac  <-  function(x) { # scaled Hobbs weeds problem -- Jacobian
  jj  <-  matrix(0.0, 12, 3)
  tt  <-  1:12
  yy  <-  exp(-x[3]*tt)
  zz  <-  1.0/(1+x[2]*yy)
  jj[tt,1]   <-   zz
  jj[tt,2]   <-   -x[1]*zz*zz*yy
  jj[tt,3]   <-   x[1]*zz*zz*yy*x[2]*tt
  attr(jj, "gradient") <- jj
  jj
}
```

## Estimation of models specified as formulas

Using a formula specification was one of the principal advances made
when `nls()` became available in S sometime in the 1980s. It uses a
Gauss-Newton (i.e., unstabilized) iteration with step reduction line
search. This works very efficiently as long as $J$ is not ill-conditioned.
In a one-parameter problem with ill-conditioning, think of $J$ as a 
very small number so that we need to watch for a zero-divide or 0/0 
situation in computing $\delta$.
The following show `nls()` does poorly on this example problem. Note that `nlsr`
offers function `pnlm0()` to give a 1-line display of class nls method results,
and `pshort()` for `nlsr`.

```{r shortforms, echo=FALSE}
library(nlsr)
```

```{r ex02nls}
unls1<-try(nls(formula=frmu, start=stu1, data=weeddf))
summary(unls1)
snls1<-try(nls(formula=frms, start=sts1, data=weeddf))
summary(snls1)
tnls1<-try(nls(formula=frmt, start=stt1, data=weeddf))
summary(tnls1)
```

Here we see the infamous "singular gradient" termination message of `nls()`. 
Questions appear quite frequently on the R-help mailing list where users
have encountered this termination result.

Let us consider three alternatives to `nls()`: `nlsr::nlxb()`, `minpack.lm::nlsLM()`
and `gslnls::gsl_nls()`.
We use default methods and controls in these examples. 

### Solution attempts with nlsr

```{r ex02nlsr}
library(nlsr)
unlx1<-try(nlxb(formula=frmu, start=stu1, data=weeddf))
print(unlx1) 
pshort(unlx1) # A short form output
snlx1<-try(nlxb(formula=frms, start=sts1, data=weeddf))
pshort(snlx1) # or print(snlx1)
tnlx1<-try(nlxb(formula=frmt, start=stt1, data=weeddf))
pshort(tnlx1) # alternatively print(tnlx1)
```

Though we have found solutions, the Jacobian is essentially singular as 
shown by its singular
values. Note that these are **displayed** by package `nlsr` in a single column in
the output to provide a compact layout, but the values do **NOT** correspond to 
the individual parameters in 
whose row they appear; they are a property of the whole problem.


### Solution attempts with minpack.lm

```{r ex02minpack}
library(minpack.lm)
unlm1<-try(nlsLM(formula=frmu, start=stu1, data=weeddf))
summary(unlm1) # Use summary() to get display
# unlm1 # Note the difference. Use this form to get sum of squares
# pnlm0(unlm1)  # Short form of output
snlm1<-try(nlsLM(formula=frms, start=sts1, data=weeddf))
pnlm0(snlm1)
# summary(snlm1)
## Following structure needed to deal with M1mac issue
tnlm1<-try(nlsLM(formula=frmt, start=stt1, data=weeddf))
if (inherits(tnlm1, "try-error")) {
   cat("Failure to compute solution -- likely singular Jacobian\n")
} else {  
   pnlm0(tnlm1) # short form to give sum of squares, else use summary(tnlm1)
}   
```

### Solution attempts with gslnls

```{r ex02gslnls}
library(gslnls)
ugslnls1<-try(gsl_nls(fn = frmu, data = weeddf,  start = stu1))
summary(ugslnls1) # Use summary() to get display
pnlm0(ugslnls1) # to get sum of squares
sgslnls1<-try(gsl_nls(fn = frms, data = weeddf,  start = sts1))
pnlm0(sgslnls1) # Use summary() to get display
tgslnls1<-try(gsl_nls(fn = frmt, data = weeddf,  start = stt1))
pnlm0(tgslnls1) 
```

### Comparison notes - formula setup

`nlsr::nlxb()` is set up to use `print()` to output standard errors and 
singular values of the Jacobian (for diagnostic purposes). By contrast, 
`minpack.lm::nlsLM()` and `nls()`
use `summary()`, which does NOT display the sum of squares, while `print()` 
gives the sum of squares, but not the standard error of the residuals.

The singular values displayed by `print.nlsr()` (the internal name for the 
adaptation of the generic `print()`) are displayed in a column to the right 
of the coefficient  and standard error display, but are NOT specific to the 
parameters. Their position is purely for efficient use of page space. The 
most common use of the singular values is to gauge how "nearly singular" the 
Jacobian is at the solution, and the ratio of the largest to smallest of the 
singular values is a simple but effective measure. In the above example, we note 
that the scaled logistic has the smallest ratio. 

The results from `nlsLM` and `gsl_nls` for the transformed model have a very 
large sum of squares, which may suggest that these program have failed. 
Since `nls()`, `nlsLM()`, and `gsl_nls()` do not 
offer the singular values, we need to extract the Jacobian and compute its
singular values. The following script shows how to do this, using what is called
the `gradient` element in the returned solution for these solvers.

```{r ex04singval, echo=TRUE, eval=TRUE}  
# for nlsLM
if (inherits(tnlm1, "try-error"))  {
   print("Cannot compute solution -- likely singular Jacobian")
 } else {  
   JtnlsLM <- tnlm1$m$gradient() # actually the Jacobian
   svd(JtnlsLM)$d # Singular values
}   
# for gsl_nls
if (inherits(tgslnls1, "try-error")) {
   cat("Cannot compute solution -- likely singular Jacobian")
} else {  
   JtnlsLM <- tgslnls1$m$gradient()
   svd(JtnlsLM)$d # Singular values
}   
```

We see that there are differences in detail, but the more important result is that
two out of three singular values are essentially 0. Our Jacobian is singular, and no 
method of the Gauss-Newton type should be able to continue. Indeed, from this set of
parameters, `nlxb` also stops, even though the code in `nlsr` tries to find solutions
from very poor initial parameter values. 

```{r ex05, echo=TRUE}  
stspecial<- c(Asym = 35.532,  xmid = 43376,  scal = -2935.4)
badstart<-nlxb(formula=frmt, start=stspecial, data=weeddf)
print(badstart)
```

## Functional specification of problems

We illustrate how to solve nonlinear least squares problems using
a function to define the residual. Note that `gsl_nls()` requires
a vector `y` that is the expected value of what we have called the
residual, but in this case is actually the model. `gsl_nls` uses 
a numerical approximation for the Jacobian if the argument `jac` 
is missing.

```{r exhobbsfn, eval=TRUE}
require(minpack.lm)
require(nlsr)
require(gslnls)
hobnlfb<-nlfb(start=stu1, resfn=hobbs.res, jacfn=hobbs.jac)
print(hobnlfb)
hobnlm<-nls.lm(par=stu1, fn=hobbs.res, jac=hobbs.jac)
summary(hobnlm)  
hobgsln<-gsl_nls(start=stu1, fn=hobbs.res, y=rep(0,12))
summary(hobgsln)
hobgsl<-gsl_nls(start=stu1, fn=hobbs.res, y=rep(0,12),jac=hobbs.jac)
summary(hobgsl)
```

## Design goals, termination tests and output objects

The output object of `nlxb()` is smaller than the class `nls` object returned
by `nls()`, `nlsLM()` and `gsl_nls()`. Package `nlsr` emphasizes the solution 
of the nonlinear least squares problem rather than the estimation of a nonlinear 
model that fits or explains the data. `nls()` and `nlsLM` return an object of 
class `nls`, which allows for a number of specialized modeling and diagnostic extensions. For compatibility, 
the `nlsr` package has function `wrapnlsr()`, for which `nlsr()` is an alias. 
This uses `nlxb()` to find good parameters, then calls `nls()` to return the class 
`nls` object. Unless particular modeling features are needed, the use of 
`wrapnlsr()` is unnecessary and wasteful of resources.

The design goals of the different tools are also evident in the so-called
"convergence tests" for the iterative solvers. In the manual page for `nls()` 
in R 4.0.0 there was the warning:

>  **Do not use `nls` on artificial "zero-residual" data.**

and suggested adding small perturbations to the data. This amounted to 
admitting `nls()` cannot solve well-posed problems unless data is polluted
with errors. We dispute that zero-residual problems are always artificial,
since problems in function approximation and nonlinear equations can be 
approached with nonlinear least squares. Moreover, the matter can
can be easily resolved. The "termination test" for the **program** rather than for 
"convergence" of the underlying **algorithm** is the Relative Offset Convergence
Criterion (see @BatesWatts81). This projects the proposed step in the parameter
vector on the gradient and estimates how much the sum of squares loss function 
should decrease. This estimate is divided 
by the current size of the loss function to avoid scale issues.
When we have "converged", the estimated
decrease is very small. However, with small residuals,
the sum of squares loss function is (almost) zero and we get the possibility of a 
zero-divide failure. 

Adding a small quantity to the loss function before dividing avoids trouble. 
In 2021, one of us (J. Nash) proposed that `nls.control()` have an additional parameter `scaleOffset` with a default value of zero. Setting it to a small 
number -- 1.0 is a reasonable choice -- allows small-residual problems 
(i.e., near-exact fits) to be dealt with easily. We call this the
**safeguarded relative offset convergence criterion**. The default value 
gives the legacy behaviour.
We are pleased to note that this improvement is now in the R distributed code 
since version 4.1.0, and it has been a part of `nlsr` since it was introduced.

More general termination tests can be used. `nlsr` a **small sum of 
squares** test (**smallsstest**) that compares the latest evaluated sum of squared
(weighted) residuals to `e4` times the initial sum of squares, where
`e4 <- (100*.Machine$double.eps)^4` is approximately 2.43e-55.

Termination after what may be considered excessive computation is also important.
`nls()` stops after `maxiter` "iterations".  
Unfortunately, the meaning of "iteration" varies among programs and requires 
careful examination of the
code. We prefer to use the number of times the residuals or the jacobian have
been computed and put upper limits on these. Our codes exit (terminate) 
when these limits are reached. Generally we prefer larger limits than the 
default `maxiter = 50` of `nls()`,
but that may reflect the more difficult problems we have encountered because 
users consult us when standard tools have given unsatisfactory results. 

### Returned results of nls() and other tools

The output of `nls()` is an object of class "nls" which has the following structure:

>A list of

>`m` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;  an `nlsModel` object incorporating the model.

>`data`	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;the expression that was passed to `nls` as the data argument. The actual \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$  data values are present in the environment of the `m` components, e.g., \newline $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ `environment(m$conv)`.

>`call` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the matched call with several components, notably `algorithm`.

>`na.action` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the `"na.action"` attribute (if any) of the model frame.

>`dataClasses` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $~$the `"dataClasses"` attribute (if any) of the "`terms`" attribute of the \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ model frame.

>`model` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;if `model = TRUE`, the model frame.

>`weights` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if `weights` is supplied, the weights.

>`convInfo` &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; a list with convergence information.

>`control` &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; the control `list` used, see the `control` argument.

>`convergence, message` &nbsp;for an `algorithm = "port"` fit only, a convergence code (`0` for  \newline
$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$convergence) and message.

> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; To use these is *deprecated*, as they are available from `convInfo` now.

                  -----------------------------------------------------------

The complexity of this object is a challenge to users. Let us use 
`result` as the returned object from `nls()`, `minpack.lm::nlsLM()` or
`gslnls::gsl_nls()`. For example, the `data` return element is an R symbol. To
actually access the data from this element, we need to use the syntax:

```
eval(parse(text=result$data))
```

However, if the call is made with `model=TRUE`, then there is a returned element
`model` which contains the data, and we can list its contents using:

```
ls(result$model)
```

and if there is an element called `xdata` that can be accessed as
`result$model$xdata`.

By contrast `nlsr::nlxb()` returns

>`coefficients`    A named vector giving the parameter values at the supposed solution.

>`ssquares` $~~~~~$ The sum of squared residuals at this set of parameters.

>`resid` $~~~~~~~~~~$ The residual vector at the returned parameters.

>`jacobian` $~~~~~$ The jacobian matrix (partial derivatives of residuals w.r.t. the 
                parameters) at the \newline
$~~~~~~~~~~~~~~~~~~~~$ returned parameters.

>`feval` $~~~~~~~~~~$ The number of residual evaluations (sum of squares computations) used.

>`jeval` $~~~~~~~~~~$ The number of Jacobian evaluations used.


However, actually looking at the structure of a returned result gives a list of 11
items. The extra 5 are:

```
 $ lower       : num [1:3] -Inf -Inf -Inf
 $ upper       : num [1:3] Inf Inf Inf
 $ maskidx     : int(0) 
 $ weights     : NULL
 $ formula     :Class 'formula'  language y ~ Asym/(1 + exp((xmid - tt)/scal))
  .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv> 
 - attr(*, "class")= chr "nlsr"
```
The result object from `nlsr::nlxb()` is still much smaller than the one `nls()` 
returns. Moreover, `nlxb`
explicitly returns the sum of squares as well as the residual vector and Jacobian. 
The counts of evaluations are also returned.

### When to compute ancillary information

Tools that produce a class `nls` output object create a rich set of functions
and structures that are then used in a variety of modeling tasks. By contrast,
`nlsr` computes quantities as they are requested or needed, with additional
features in separate functions. For example, the singular values of the Jacobian 
are actually computed in the `print` and `summary` methods for the result. 
These two approaches
lead to different consequences for performance and how features are
provided. `nlsr` has antecendants in the methods of @cnm79, where storage for
data and programs was at a ridiculous premium in the small computers of the era.
Thus the code in `nlsr` is likely of value for workers to copy and modify it
for customized tools.

## Jacobian calculation

Gauss-Newton/Marquardt methods all need a Jacobian matrix at each iteration. 
By default, `nlsr::nlxb()` will try to evaluate this using analytic 
expressions using symbolic and automatic differentiation tools. 
When using a formula specification of the model, `nls()`, `minpack.lm::nlsLM()`
and `gslnls::gsl_nls()` use a finite difference approximation to compute the
Jacobian, though `gsl_nls()` does have an option to attempt symbolic expressions.
Package `nlsr` provides, via appropriate calling syntax, four numeric approximation 
options for the Jacobian, with a further control `ndstep` for the size of the
step used in the approximation. 

Using the "gradient" attribute of the output of the Jacobian function to hold
this matrix lets us embed this attribute of the **residual** function as well,
so that the call to `nlfb()` can be made with the same name used for 
both residual and Jacobian function arguments. This programming trick saves a 
lot of trouble for the package developer, but it can be a nuisance for users 
trying to understand the code. 

As far as we can understand the logic in `nls()`, the Jacobian computation during
parameter estimation is carried out entirely within the called C-language program,
and the R code function `numericDeriv()`, part of `./src/library/stats/R/nls.R`
in the R distribution source code. This is used to provide Jacobian information in 
the `nlsModel()` and `nlsModel.plinear()` functions, which are **not** exported for 
general use. `gsl_nls()` also appears to use `numericDeriv()`. 

`numericDeriv()` uses a 
simple forward difference approximation of derivatives, though a central 
difference approximation can be specified in control parameters. 
We are unclear why `numericDeriv()` in base R calls `C_numeric_deriv`,
as we were easily able to create a more compact version entirely in R. 
See *https://github.com/nashjc/RNonlinearLS/tree/main/DerivsNLS*.

`minpack.lm::nlsLM()` invokes `numericDeriv()` in its local 
version of `nlsModel()`, but it appears to use an internal approximate Jacobian 
code from 
the original Fortran `minpack` code, namely, `lmdif.f`. Such differences in approach
can lead to different behaviour, though in our experience the differences are usually
very minor.

Minor numerical differences can give annoying results with ill-conditioned
problems. 

- A pasture regrowth problem (@Huet2004, page 1, based on @Ratkowsky1983) has a
  poorly conditioned Jacobian and `nls()` fails with "singular gradient". 
  Worse, numerical approximation to the Jacobian gives the error
  "singular gradient matrix at initial parameter estimates" for `minpack.lm::nlsLM`
  so that the Marquardt stabilization is unable to take effect, while the analytic
  derivatives of `nlsr::nlxb` give a solution. 

- Karl Schilling (private communication) provided an example where a model specified
  with the formula `y ~ a * (x Ë† b)` causes `nlsr::nlxb` to fail because the partial
  derivative w.r.t. `b` is `a * (x^b * log(x))`. If there is data for which `x = 0`,
  the derivative is undefined, while the model can be computed. In such cases, 
  we observed that `nls()` and 
  `minpack.lm::nlsLM` found a solution, though this seems to be a lucky accident.

### Jacobian code in selfStart models

Analytic Jacobian code can be provided to `nls()`, `nlsLM()` and `gsl_nls()`,
and most `selfStart` models that automatically provide starting parameters
also include such code. There is documentation in R
of `selfStart` models, but the construction of them is non-trivial. A number 
of such models are included with base R in `./src/library/stats/R/zzModels.R` file 
with package `nlraa` (@MiguezNLRAA2021) providing a richer set. 
There are also some in the now-archived package `NRAIA`.
These provide the Jacobian in the "gradient" attribute of the "one-sided" formula
that defines
each model, and these Jacobians may be the analytic forms. 

The `nls()` function, after
computing the "right hand side" or `rhs` of the residual, then checks to see if the
"gradient" attribute is defined, and, if not, uses `numericDeriv` to compute a 
Jacobian
into that attribute. This code is within the `nlsModel()` or `nlsModel.plinear()`
functions. The use of analytic Jacobians
almost certainly contributes to the good performance of `nls()` on `selfStart` 
models.

The use of `selfStart` models with
`nlsr` is described in the "Introduction to nlsr" vignette. However, since `nlsr`
generally can use very crude starting values, we have rarely needed them, though
it should be pointed out that our work is primarily diagnostic. If we were carrying
out a large number of similar estimations, then good initial parameters are 
critical to efficiency.

In considering `selfStart` models, we noted that the base-R function `SSlogis`
is intended to solve our example problem **Logistic3T**, but that it finds initial 
parameters for `nls()` by using a completely separate solver, then reruns the
solution with the default algorithm from the derived -- and final -- parameters.
To provide simpler estimates of starting parameters, the function `SSlogisJN`
is now part of package `nlsr`.

The `SSLogis` case is one where the `getInitial()` function (used to find
starting values) actually calls `nls()` with a non-default algorithm. 
Such circular references are, in our view, prone to creating errors
in code maintenance.
Users may also want to use these models with explicit starting values 
other than those suggested by `getInitial()`.
We are also surprised that the 
analytic expressions for the Jacobian ("gradient") in the `SSLogis` R function 
saves many quantities in "hidden"
variables, i.e., with names preceded by ".". 
These are then not displayed by the `ls()`
command, making them difficult to access by users who may wish to create
their own selfStart model by a copy and edit.
Interactive tools, such as "visual fitting" (@nash1996nonlinear) might be
worth considering as a way to find starting parameters, but we know of no
R capability of this type.

As a side note, the introduction of `scaleOffset` in R 4.1.1 to deal with the 
convergence test for small residual problems now requires that the `getInitial()`
function have dot-arguments (`...`) in its argument list. This illustrates the
entanglement of many features in `nls()` that complicate its maintenance and
improvement.

## Bounds constraints on parameters

?? Arkajyoti -- we can probably trim these examples

In some cases, we know that parameters cannot be bigger or smaller than some
externally known limits. Cash reserves cannot be negative. Animals have a 
minimum need for water. Airplanes cannot carry more than a known or 
legislated cargo weight. Such limits can be built into models, but there 
are some important details for using the tools in R.

- `nls()` can only impose bounds if the `algorithm="port"` argument is 
used in the call. Unfortunately, the documentation warns us:

  *The algorithm = "port" code appears unfinished, and does not even 
   check that the starting value is within the bounds. Use with 
   caution, especially where bounds are supplied.*

- `gsl_nls()` does not offer bounds.

- bounds are part of the default method for package `nlsr`.

- `nlsLM()` includes bounds in the standard call, but we have observed cases
   where it fails to get the correct answer. From examination of the code,
   we believe the authors have not taken into account all possibilities, though
   it should be noted that all programs have some weakness in
   regard to constrained optimization. Software creators have to work with
   assumptions on the extremity of scale that they are willing to countenance,
   and sometimes problems will be outside the scope envisaged.

```{r ex10, echo=TRUE} 
# Start MUST be feasible i.e. on or within bounds
anlshob1b <- nls(frms, start=sts1, data=weeddf, lower=c(0,0,0),
             upper=c(2,6,3), algorithm='port')
pnls0(anlshob1b) #  check the answer (short form)
# nlsLM seems NOT to work with bounds in this example
anlsLM1b <- nlsLM(frms, start=sts1, data=weeddf, lower=c(0,0,0), upper=c(2,6,3))
pnls0(anlsLM1b)
# also no warning if starting out of bounds, but gets good answer!!
st4<-c(c1=4, c2=4, c3=4)
anlsLMob <- nlsLM(frms, start=st4, data=weeddf, lower=c(0,0,0), upper=c(2,6,3))
pnls0(anlsLMob)
# Try nlsr::nlxb()
anlx1b <- nlxb(frms, start=sts1, data=weeddf, lower=c(0,0,0), upper=c(2,6,3))
pshort(anlx1b)
```

### Philosophical considerations

Bounds on parameters raise some interesting and difficult questions about how 
uncertainty in parameter estimates should be computed or reported. That is, the
traditional "standard errors" are generally taken to imply symmetric intervals about
the point estimate in which the parameter may be expected to be found with some
probability under certain assumptions.
Bounds change those assumptions. Hence, `nlsr::nlxb()` does not compute standard
errors nor their derived statistics when bounds are active.

### Fixed parameters (masks)

Let us try to fix (mask) the first parameter in the first two example problems.

```{r ex10m, echo=TRUE}
# Hobbsmaskx.R -- masks with formula specification of problem
require(nlsr)
require(minpack.lm)
stu <- c(b1=200, b2=50, b3=0.3) # a default starting vector (named!)
sts <- c(c1=2, c2=5, c3=3) # a default scaled starting vector (named!)
traceval<-FALSE
# fix first parameter
anxbmsk1 <- try(nlxb(frmu, start=stu, data=weeddf, lower=c(200,0,0), 
			upper=c(200, 60, 3), trace=traceval))
print(anxbmsk1)
anlM1 <- try(nlsLM(frmu, start=stu, data=weeddf, lower=c(200,0,0), 
			upper=c(200, 60, 3), trace=traceval))
summary(anlM1)
# nls gives warnings
anlsmsk1 <- try(nls(frmu, start=stu, data=weeddf, lower=c(200,0,0), 
		upper=c(200, 60, 3),  algorithm="port", trace=traceval))
summary(anlsmsk1)
# Hobbs scaled problem with bounds, formula specification
anlxmsks1 <- nlxb(frms, start=sts, data=weeddf, lower=c(2,0,0),
                  upper=c(2,6,30))
print(anlxmsks1)
anlshmsk1 <- nls(frms, start=sts, trace=traceval, data=weeddf, lower=c(2,0,0),
             upper=c(2,6,3), algorithm='port')
summary(anlshmsk1)
cat("More precisely...crossprod(resid(anlshmsk1))=",crossprod(resid(anlshmsk1)),"\n")
# nlsLM does not always work with bounds
anlsLMmsks1 <- nlsLM(frms, start=sts, data=weeddf, lower=c(2,0,0),
                 upper=c(2,6,3))
summary(anlsLMmsks1)

# Test with all parameters masked
anlxmskall<- try(nlxb(frms, start=sts, data=weeddf, lower=sts, upper=sts))
print(anlxmskall)
```

`nlsr` has an output format that indicates the constraint status of the parameter
estimates. For `nlsr` we have **chosen** to suppress calculation of 
approximate standard 
errors in the parameters when constraints are active because their meaning
under constraints is unclear, though we believe this 
policy worthy of discussion and further investigation.

## Stabilization of Gauss-Newton computations

All four major tools illustrated must solve some variant of the Gauss-Newton 
equations. `nls()` uses a modification of an approach suggested by @Hartley1961,
while `nlsr`, `gslnls` and `minpack.lm` use flavours of @Marquardt1963. `gslnls`
offers an accelerated Marquardt method and three alternative methods. We have 
not explored all five methods with various control settings.
Though it is unlikely to be useful in general, 
control settings for `nlxb()` or `nlfb()` allow for those routines to perform 
a variety of Hartley and Marquardt algorithms. This
has been useful for learning about the algorithms, but tangential to simply finding
parameters of nonlinear models.

In general, the Levenberg-Marquardt stabilization is important in obtaining 
solutions in methods of the Gauss-Newton family, as `nls()` terminates 
too frequently and unnecessarily with `singular gradient` errors.


### Programming language

An important choice made in developing `nlsr` was to code entirely within the R 
programming language. `nls()` uses a mix of R, C and Fortran, as does `minpack.lm`. 
`gslnls` is an R wrapper to various C-language routines in the Gnu Scientific 
Library (@GSL-manual).
Generally, we believe that keeping to a single programming language can allow for 
easier maintenance and upgrades. It also avoids some work when there are changes 
or upgrades to libraries for the non-R languages. 
On the other hand, R is usually somewhat slower 
than some other computing systems because it keeps track of names and structures and 
because it is usually interpreted rather than compiled. In recent years the 
performance penalty for using code entirely in R has been much reduced
with the just-in-time compiler and other improvements, so that using an all-R 
package offers acceptable performance. Indeed, in `nlsr` the use of R may be 
less of a performance cost than a policy of aggressively seeking a solution,
which can cause more iterations to be used.

## Data sources to problems

`nls()` can be called without specifying the `data` argument. In this case, it will
search in the available environments (i.e., workspaces) for suitable data objects. 
We do NOT like this approach, but it is "the R way". R allows users to leave many 
objects in the default (`.GlobalEnv`) workspace. Moreover, users have to actively 
suppress saving this workspace (`.RData`) on exit; otherwise, any such file in 
the path, 
when R is launched, will be loaded. The overwhelming proportion of R users in our
acquaintance avoid saving the workspace because of the danger of lurking data and
functions which may cause unwanted results.

Nevertheless, to provide compatible behaviour with `nls()` and R generally, 
programs need to ensure equivalent behaviour, but users should test that the 
operation is as they intend. 

### Feature: Subsetting

`nls()` and other class `nls` tools accept an argument `subset`. This acts through the mediation of `model.frame` and is not clearly obvious in the source code files `/src/library/stats/R/nls.R` and `/src/library/stats/src/nls.C`. 

Having `subset` at the level of the call to a function like
`nls()` saves effort, but it does mean that the programmer of the 
solver needs to be aware of the
source (and value) of objects such as the data, residuals and Jacobian. 
This is overly complicated. By preference, 
we would implement subsetting by means of zero-value weights, with observation counts
(and degrees of freedom) computed via the numbers of non-zero weights. Alternatively,
we would extract a working dataframe from the relevant elements in the original.

### Feature: na.action

`na.action` is an argument to the `nls()` function, but it does not appear obviously in the source code, often being handled behind the scenes after referencing the option `na.action`. Yet this feature also changes the data supplied to our nonlinear
least squares solver. 

A useful, but possibly dated, description is given in:
*https://stats.idre.ucla.edu/r/faq/how-does-r-handle-missing-values/*.
The typical default action, which can be seen by using the command
`getOption("na.action")`
is `na.omit`. This option essentially omits from computations any observations
containing missing values (i.e. any row of a data frame containing an NA). 
`na.exclude` does much of the same for computations, but keeps the rows with NA 
elements so that predictions are in the correct row position. We recommend that 
workers actually test output to verify the behaviour is as wanted.
See *https://stats.stackexchange.com/questions/492955/should-i-use-na-omit-or-na-exclude-in-a-linear-model-in-r*.

As with `subset`, our concern with `na.action` is that users may be unaware of the
effects of an option they may not even be aware has been set. Should `na.fail` be the default?

## Feature: model frame

`model` is an argument to the `nls()` and related functions, which is documented as:

> **model** logical. If true, the model frame is returned as part of the object. Default is FALSE.

Indeed, the argument only gets used when `nls()` is about to return its result
object, and the
element `model` is NULL unless the calling argument `model` is TRUE. (Using the same 
name could
be confusing.) However, the model frame is used within the function code in the form 
of the object
`mf`. We feel that users could benefit from more extensive documentation and 
examples of its use since it is used to implement features like `subset`.


## Weights on observations

All four main tools we consider here allow a `weights` argument that
specifies a vector of fixed weights the same length as the number of residuals. 
Each residual is multiplied by the square root of the corresponding weight. 
Where available, the values returned by the
`residuals()` function are weighted, and the `fitted()` or `predict()` function are 
used to compute raw residuals.

While fixed weights may be useful, there are a number of problems for which
we want weights that are determined at least partially from the model 
parameters, for
example, a measure of the standard deviation of observations.
Such dynamic weighting situations are discussed in the vignette 
"Introduction to nlsr" of package `nlsr` in section 
*Weights that are functions of the model parameters*. Care is advised
in applying such ideas.



### Weights in returned functions from nls()

The function `resid()` (an alias for `residuals()`) gives WEIGHTED residuals, 
as does, for example, `result$m$resid()`. The function `nlsModel()`, which we 
have had to extract from the base R code and explicitly `source()` because it 
is not exported to the working namespace,
allows us to compute residuals for particular coefficient sets.

```{r nlswtx, echo=TRUE}
weed <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972)
tt <- 1:12
weeddf <- data.frame(tt, weed)
wts <- 0.5^tt # simple weights
frmlogis <- weed ~ Asym/(1 + exp((xmid - tt)/scal))
Asym<-1; xmid<-1; scal<-1
nowt<-nls(weed ~ SSlogis(tt, Asym, xmid, scal)) # UNWEIGHTED
nowt
nowt$m$resid() # This has UNWEIGHTED residual and Jacobian. Does NOT take coefficients.
usewt <- nls(weed ~ SSlogis(tt, Asym, xmid, scal), weights=wts)
usewt
usewt$m$resid() # WEIGHTED. Does NOT take coefficients.
source("nlsModel.R")
nmod0 <- nlsModel(frmlogis, data=weeddf, start=c(Asym=1, xmid=1, scal=1), wts=wts)
nmod0$resid() # Parameters are supplied in nlsModel() `start` above.
nmod <- nlsModel(frmlogis, data=weeddf, start=coef(usewt), wts=wts)
nmod$resid()
```

## Minor complaints

### Interim output from the "port" algorithm

As the `nls()` **man** page states, when the "port" algorithm is used with the
`trace` argument TRUE, the iterations display the objective function value which 
is 1/2 the sum of squares (or deviance). It is likely that the trace display is
embedded in the Fortran of the `nlminb` routine that is called to execute 
the "port" algorithm, but the factor of 2 discrepancy is nonetheless
unfortunate for users. 

### Failure to return best result achieved

If `nls()` reaches a point where it cannot continue but has not found a point 
where the relative offset convergence criterion is met, it may simply exit,
especially if a "singular gradient" (singular Jacobian) is found. However, 
this may occur AFTER the function has made considerable progress in reducing 
the sum of squared residuals. 
<!-- An example is to be found in the `Tetra_1.R` example from the `nlsCompare` package. -->
<!-- ?? AB: Are we far enough along to include reference? -->
Here is an abbreviated example:

```{r tetrarun}
time <- c( 1,  2,  3,  4,  6 , 8, 10, 12, 16)
conc <- c( 0.7, 1.2, 1.4, 1.4, 1.1, 0.8, 0.6, 0.5, 0.3)
NLSdata <- data.frame(time,conc)
NLSstart <- c(lrc1 = -2, lrc2 = 0.25, A1 = 150, A2 = 50) # a starting vector (named!)
NLSformula <- conc ~ A1 * exp(-exp(lrc1) * time) + A2 * exp(-exp(lrc2) * time)
tryit <- try(nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE))
print(tryit)
```

Note that the sum of squares has been reduced from 61216 to 1.6211, but 
unless `trace` is invoked, the user will not get any information about this.
This almost trivial change to the `nls()` function could be 
useful to R users.

## Estimating models that are partially linear

Many nonlinear models can be expressed so that some of the parameters 
appear linearly. In our logistic examples, the asymptote parameters are
an illustration. In such cases, specialized methods can give spectactularly
better performance. The variable projection method (@Golub1973, @OlearyRust13) is 
generally much more effective than general approaches in finding good solutions 
to nonlinear least squares 
problems when some of the parameters appear linearly. However, setting up the 
calculations, that is, identifying which parameters are linear, is not a trivial
task. 

`nls()` has an option `algorithm="plinear"` that allows some partially linear
models to be solved. The other tools, as far as we are aware, do not offer any
such capability, with no external package providing it. 
The feature in `nls()` has, however, some awkwardness.
In particular, we would like to specify a model to a solver consistently
across solver methods. Unfortunately, within `nls()` itself we must use
different specifications with different `algorithm` options.

The nonlinear model specifications are, of course, a development of linear ones.
Unfortunately, the explicit model `y ~ a * x + b` does not work with the linear
modeling function `lm()`, which requires this model to be specified as `y ~ x`.
Within `nls()`, consider the following FOUR different specifications for the same 
problem, plus an intuitive choice, labelled `fm2a`, that does NOT work. 
In this failed attempt, putting the `Asym` parameter in the model causes the
`plinear` algorithm
to try to add another term to the model. We believe this is unfortunate, and would
like to see a consistent syntax. At the time of writing, we do
not foresee a resolution for this issue. In the example, we have NOT evaluated
the commands to save space.

```{r log4ways, echo=TRUE, eval=FALSE}
DNase1 <- subset(DNase, Run == 1) # select the data
## using a selfStart model - do not specify the starting parameters
fm1 <- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
summary(fm1)

## using conditional linearity - leave out the Asym parameter
fm2 <- nls(density ~ 1 / (1 + exp((xmid - log(conc)) / scal)),
                 data = DNase1, start = list(xmid = 0, scal = 1),
                 algorithm = "plinear")
summary(fm2)

## without conditional linearity
fm3 <- nls(density ~ Asym / (1 + exp((xmid - log(conc)) / scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1))
summary(fm3)

## using Port's nl2sol algorithm
fm4 <- try(nls(density ~ Asym / (1 + exp((xmid - log(conc)) / scal)),
                 data = DNase1, start = list(Asym = 3, xmid = 0, scal = 1),
                 algorithm = "port"))
summary(fm4)

## using conditional linearity AND Asym does NOT work
fm2a <- try(nls(density ~ Asym / (1 + exp((xmid - log(conc)) / scal)), 
                 data = DNase1, start = list(Asym=3, xmid = 0, scal = 1),
                 algorithm = "plinear", trace = TRUE))
summary(fm2a)
```


## Models with indexed parameters

Indexed parameters -- those whose names have subscripts on some element that
is part of the coefficient list -- 
are a useful idea to simplify model specification in some cases. However, they
are also a focus of trouble. Typically we have some common parameters and 
others that are tied to particular cases.

The **man** file for `nls()` includes the following example of a situation in which
parameters are indexed. It also uses the "plinear" option as an added complication.

Here we use a truncated version of the example to save display space.

```{r nlsindx1}
## The muscle dataset in MASS is from an experiment on muscle
## contraction on 21 animals.  The observed variables are Strip
## (identifier of muscle), Conc (Cacl concentration) and Length
## (resulting length of muscle section).
if(! requireNamespace("MASS", quietly = TRUE)) stop("Need MASS pkg")
mm<- MASS::muscle[1:12,] # take only 1st few values of Strip (TRUNCATION OF EXAMPLE)
mm<-droplevels(mm) # remove unused levels after truncation
nlev <- nlevels(mm)
withAutoprint({
  ## The non linear model considered is
  ##       Length = alpha + beta*exp(-Conc/theta) + error
  ## where theta is constant. For now alpha and beta do NOT vary with Strip.
  with(mm, table(Strip)) # 2, 3 or 4 obs per strip
  nl <- nlevels(mm$Strip)
  ## We first use the plinear algorithm to fit an overall model,
  ## ignoring that alpha and beta might vary with Strip.
  musc.1 <- nls(Length ~ cbind(1, exp(-Conc/th)), mm,
                start = list(th = 1), algorithm = "plinear")
  summary(musc.1)

  ## Then we use nls' indexing feature for parameters in non-linear
  ## models to use the conventional algorithm to fit a model in which
  ## alpha and beta vary with Strip.  The starting values are provided
  ## by the previously fitted model.
  ## Note that with indexed parameters, the starting values must be
  ## given in a list (with names):
  b <- coef(musc.1)
  musc.2 <- nls(Length ~ a[Strip] + b[Strip]*exp(-Conc/th), data=mm,
                start = list(a = rep(b[2], nl), b = rep(b[3], nl), th = b[1]))
  summary(musc.2)
})
```

Running the example reveals that the answers for the parameters are NOT indexed. 
<!-- e.g. `coef(musc.2)` is a single level vector of parameters.  -->
That is, we do not see `a[1], a[2], a[3]` but `a1, a2, a3`. 
<!-- This is because the model -->
<!-- must integrate all the parameters because `th` is a common parameter across the index `Strip`. -->
This is no doubt because programming for indexed parameters is extremely challenging.
<!-- However, we believe the current structure could cause confusion and error, and  -->
<!-- propose an alternative approach below. -->

If such models are of enough importance, we believe it would be relatively
straightforward to prepare an external package to provide for their estimation,
including an appropriately formatted output object so the indexing of the 
parameters can be used in other calculations.

## Tests and use-case examples

Maintainers of packages need suitable tests and use-case examples in order:

- to ensure packages work properly, in particular, giving
  results comparable to or better than the functions they are to replace;
- to test individual solver functions to ensure they work across the range of 
  calling mechanisms, that is, different ways of supplying inputs to the solver(s);
- to pose "silly" inputs to see if these bad inputs are caught by the programs.
  
Such goals align with the aims of **unit testing** 
(e.g., *https://towardsdatascience.com/unit-testing-in-r-68ab9cc8d211*, @HWtestthat11, @HWdevtools21
and the conventional R package testing tools).
 
In our work, one of us has developed a working prototype package at *https://github.com/ArkaB-DS/nlsCompare* .
A primary design objective of this is to allow the summarisation of multiple tests in
a compact output. The prototype has a vignette to illustrate its use.

## Documentation and resources

In our investigation, we built several resources, which are now part of 
the repository *https://github.com/nashjc/RNonlinearLS/*. Particular items
are:

- A BibTex bibliography for use with all documents in this project, but which 
  has wider application to nonlinear least squares projects in general
  (*https://github.com/nashjc/RNonlinearLS/blob/main/BibSupport/ImproveNLS.bib*).

- `MachID.R` offers a suggested concise summary function
  to identify a particular computational system used for tests. A discussion 
  of how it was built and the resources needed across platforms is given in
  at *https://github.com/nashjc/RNonlinearLS/tree/main/MachineSummary*.

- @PkgFromRbase22 is an explanation of the construction of the `nlspkg` from the
  `nls()` code in R-base. 

- As the 2021 Summer of Code period was ending, one of us (JN) was invited to prepare
  a review of optimization in R. Ideas from the present work have been instrumental
  in the creation of @NashWires22.

## Should nls() be refactored?

When we started *Improvements to nls()* for the 2021 Google Summer of Code we wanted:

- to provide a packaged version of `nls()` (call it `nlsalt`) coded entirely in R 
  that matches the features in base R or what is packaged in `nlspkg` as described 
  in @PkgFromRbase22. Our intent in providing a purely R version of the code was
  either to replace `nls()` or to serve as a reference version aiding maintenance 
  of C, Fortran, or other components of the software.
  
- to streamline the overall `nls()` infrastructure. By this we mean a 
  refactoring of the routines so they are better suited to maintenance of both
  the existing `nls()` methods and features as well as new features we or others
  would like to add.

- to explain what we do, either in comments, examples, or separate maintainer 
  documentation. While R and its packages are generally very well documented for
  usage, that documentation
  rarely extends as far as it might for code maintenance. The paucity of such
  documentation is exacerbated by the mixed R/C/Fortran code base,
  where seemingly trivial differences in things like representation of numbers or
  base index for arrays lead to traps for unwary programmers.

  
These goals echo themes in the recent discussion by @Vidoni21 and accompanying 
commentary.

A serious obstacle to changing `nls()` is that there are many legacy applications
and examples. Furthermore, with `nls()` as part of the base R distribution, users
are drawn to using it even when it is less than ideal. A possible way forward
is to 

- use a packaged version of `nls()` to provide legacy features, with notification
  to users that there are other tools available

- if maintainers are willing, to merge and rationalize the capabilities of 
  `minpack.lm`, `nlsr` and `gslnls`

- to encourage the (separate) development of capabilities such as indexed
  parameter models if these are in demand.
  
This direction does not, of course, resolve the inconsistencies in model 
specification in formulas for nonlinear models, which undoubtedly is an issue
for other modeling tools.  

A possible workaround to allow consistency for the `plinear` algorithm 
would be to specify the linear parameters when the `plinear` option is specified. 
For example, we could use `algorithm="plinear(Asym)"` while keeping the general 
model formula from the default specification. This would allow 
for the output of different `algorithm` options to be consistent. However, we 
have not yet tried to code such a change, and welcome discussion from those working
with other packages using model formulas.

A further complication of model specification in R is that some formulas require
the user to surround a term with `I()` to inhibit interpretation of arithmetic 
operators. We would prefer that formulas be usable without this complication.



---OLD VERSION BELOW ------------------------

## Future of nonlinear model estimation in R

It is possible that `nls()` will remain more or less as it has been for the past
two decades. Given its importance to R, the focus of discussion should be the
measures needed to secure its continued operation for legacy purposes and how that
may be approached. We welcome an opportunity to participate in such conversations. 

To advance the stability and maintainability of R, we believe the program objects (R functions)
that are created by tools such as `nls()` should have minimal cross-linkages and side-effects.
The aspects of `nls()` that have given us the most trouble:

- The functions that compute the residuals and jacobians often presume
  the data and parameters needed are available in a particular environment. As long
  as the correct environment is used, this provides a compact syntax to invoke the
  calculations. The danger is that the wrong data is accessed if the internal search finds a
  valid name that is not the object we want.
  
- Weights, subsets, and various contextual controls (such as that for the `na.action` option)
  are similarly taken from the first available source, making for a very simple
  invocation of calculations, but the context is hidden from the user. Furthermore, it can be
  difficult for those of us trying to maintain or improve the code to be certain we have the
  context correct.
  
- The mixing of R, C and Fortran code was necessary for computational performance in the past.
  Lacking decent developer documentation, programmers now face a lot of work to fix or improve code. 
  We believe in having at least a working reference version of code in a single programming language. 
  If necessary, by
  measuring ("profiling") the code, we can find bottlenecks and substitute for those slower 
  parts of the reference code.

- A design that isolates the setup, solution, and post-solution parts of
  for complicated calculations reduces the number of objects that must be kept in 
  alignment. On the other hand, it may mean users have to add more commands to complete
  their computations.
  
- All iterative codes should return the best solution they have found so far, even if there
  are untoward conditions reached, such as a singular Jacobian. This modification could be
  made to `nls()` in the short term, correcting the issue identified above under "Failure to
  return best result achieved".

- Given the existence of examples of good practices such as analytic derivatives, stabilized 
  solution of Gauss-Newton equations and bounds-constrained parameters, base R tools should 
  be moving to incorporate them. 

We anticipate that tools like `nlsr` will become more attractive as users discover
the increased robustness and general capabilities, along with the more recent documentation.
From a design point of view, a key difference in approach between `nls()` and 
`nlsr::nlxb()` is that `nls()` 
builds a large infrastructure, especially the `nlsModel()` function,  from which the Gauss-Newton
iteration can be executed and other statistical information such as profiles can be
computed, while `nlxb()` returns quite limited information, and 
computes what is needed on an "as and when" basis. This follows a path that one of us
(JN) established almost 50 years ago with the software that became @cnm79, separating
setup, solver, and post-solution analysis phases of computations. There is, however,
the wrapper `wrapnlsr()` that solves the nonlinear estimation problem with the solver
of `nlsr` then calls `nls()` to build the object of class `nls` for which profile log-likelihood
information can be computed.
<!-- Here, that last  -->
<!-- phase is not part of `nlxb()`, but is the domain of other functions in the `nlsr` -->
<!-- package. -->


<!-- The `nls()` approach, as implemented in the base R code and `nlspkg` leads to considerable -->
<!-- entangling of the different functions and capabilities. Even after -->
<!-- both the preparation for and carrying out of the present project, we do not feel confident -->
<!-- to explain the code completely, nor to maintain it. However, we have made some  -->
<!-- forays into render parts of the code in R, and to understand where difficulties -->
<!-- lie. -->

<!-- ??fix next bits -->
<!-- - The different dialects of modeling expressions as detailed above should be rationalized. This -->
<!--   would, however, be a painful and mainly political activity. -->

<!-- - The tools for analytic derivatives that exist in parts of `nls()` for self-starting models and -->
<!--   in `nlsr` for problems specified as models that use expressions for which derivatives exist  -->
<!--   should be more generally available, but the task is one of tedium and limited reward to the -->
<!--   persons who do it, while benefiting the R community substantially. -->

<!-- - Similarly, default methods need stabilized solvers e.g., Marquardt methods, though to date -->
<!--   a mechanism to incorporate such modifications in `nls()` has eluded us. -->

<!-- - Bounds constraints are sufficiently important that they should be part of the default -->
<!--   methods -->
<!--   (rather than algorithm = "port" for `nls()`). `minpack.lm` should handle them correctly. -->


# Acknowledgement

Hans Werner Borchers has been helpful in developing the proposal for this project
and in comments on this and related work. Heather Turner co-mentored the project 
and helped guide the progress of the work. Exchanges with Fernando Miguez helped to
clarify aspects of `selfStart` models and instigated the "Introduction to nlsr" 
vignette. Colin Gillespie (package `benchmarkme`) has been helpful in guiding our 
attempts to succinctly summarize computing environments.


# References
